{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da038142",
   "metadata": {},
   "source": [
    " PPã‹ã‚‰locationã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e521eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import sys\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b72ad8",
   "metadata": {},
   "source": [
    "time, mesh, actã®å€‹äººã”ã¨ã®ãƒˆãƒ¼ã‚¿ãƒ«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆã“ã®å¾Œã«1é€±é–“ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c20a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip dataã®å‡ºç™ºæ™‚åˆ»ã«ä½µã›ã¦ loc_dataã‹ã‚‰lon, latã‚’å–ã‚Šå‡ºã™\n",
    "base_path0 = '/Users/matsunagatakahiro/Desktop/jrres/PPcameraTG/gpslog'#/04_202212old'\n",
    "input_path = '/Users/matsunagatakahiro/Desktop/res2025/ActFormer/RoutesFormer/actgpt0512/input'\n",
    "dataset_list = ['04_202212old', '04_202301new', '05_202311', 'toyosu_2019/201907-202002', '99_202110/logs']\n",
    "nameid_list = ['01', '02', '03', 'toyosu', 'shibu21']\n",
    "gdf_mesh1 = gpd.read_file(os.path.join(base_path0, '3rdmesh5339/MESH05339.shp'))\n",
    "gdf_mesh2 = gpd.read_file(os.path.join(base_path0, '3rdmesh5340/MESH05340.shp'))\n",
    "gdf_mesh3 = gpd.read_file(os.path.join(base_path0, '3rdmesh5440/MESH05440.shp'))\n",
    "gdf_mesh4 = gpd.read_file(os.path.join(base_path0, '3rdmesh5239/MESH05239.shp'))\n",
    "gdf_mesh5 = gpd.read_file(os.path.join(base_path0, '3rdmesh5439/MESH05439.shp'))\n",
    "\n",
    "gdf_mesh = pd.concat([gdf_mesh1, gdf_mesh2, gdf_mesh3, gdf_mesh4, gdf_mesh5], ignore_index=True)\n",
    "gdf_mesh = gdf_mesh.to_crs(\"EPSG:4326\") # WGS84ã«å¤‰æ›\n",
    "gdf_mesh['3rdmesh'] = gdf_mesh['KEY_CODE'].astype(str).str[0:6] # 3æ¬¡ãƒ¡ãƒƒã‚·ãƒ¥ã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "act_dict_shibuya2223 = {\n",
    "    110: 1, # H\n",
    "    200: 2, # S\n",
    "    210: 2, # S\n",
    "    220: 1, # hotel->home\n",
    "    300: 3, # W \n",
    "    310: 4, # W2\n",
    "    400: 3, # W\n",
    "    998: 5, # Other\n",
    "    999: 5 # , # Other\n",
    "}\n",
    "\n",
    "act_dict_shibuya21toyosu = {\n",
    "    300: 1, # H\n",
    "    201: 1, # H\n",
    "    400: 2, # S\n",
    "    401: 2, \n",
    "    402: 2, \n",
    "    404: 2, \n",
    "    405: 2, \n",
    "    406: 2, \n",
    "    100: 3, # W\n",
    "    200: 4, # W2\n",
    "    403: 5, # Ohter\n",
    "    407: 5, # Ohter\n",
    "    500: 5, # Ohter\n",
    "    501: 5, # Ohter\n",
    "    999: 5 #, # Ohter`\n",
    "}\n",
    "\n",
    "def remove_consecutive_duplicates(act_row, time_row, mesh_row):\n",
    "    # print('remove_consecutive_duplicates', act_row, time_row, mesh_row)\n",
    "    new_act = [act_row[0]]\n",
    "    new_time = [time_row[0]]\n",
    "    new_mesh = [mesh_row[0]]\n",
    "    for i in range(1, len(act_row)):\n",
    "        if not (act_row[i] == act_row[i-1] and mesh_row[i] == mesh_row[i-1]):\n",
    "            new_act.append(act_row[i])\n",
    "            new_time.append(time_row[i])\n",
    "            new_mesh.append(mesh_row[i])\n",
    "    return pd.Series(new_act), pd.Series(new_time), pd.Series(new_mesh)\n",
    "\n",
    "def try_convert_int(x):\n",
    "    if x in ['<b>', '<e>']:\n",
    "        return x\n",
    "    try:\n",
    "        return int(float(x))  # 300.0 â†’ 300 ãªã©ã«ã‚‚å¯¾å¿œ\n",
    "    except:\n",
    "        return x  # å¤‰æ›ã§ããªã‘ã‚Œã°ãã®ã¾ã¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfe4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20190915ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20190916ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20191020ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20191229ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20191230ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20191231ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200101ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200102ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200103ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200104ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200105ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200111ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200112ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200113ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200126ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200127ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200201ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200202ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200208ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200209ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200210ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200211ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "    user_log_dict = {}\n",
    "    user_count_dict = {} # ä½•æ—¥ç™»å ´ã—ãŸã‹ã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    user_mesh_traj = {}\n",
    "    user_time_traj = {}\n",
    "    user_act_traj = {}\n",
    "    nokitaku_count = 0\n",
    "    all_count = 0\n",
    "    folderlist = sorted(folderlist)\n",
    "\n",
    "    if dataset == 'toyosu_2019/201907-202002' or dataset == '99_202110/logs': \n",
    "        act_dict = act_dict_shibuya21toyosu\n",
    "    elif dataset == '04_202212old' or dataset == '04_202301new' or dataset == '05_202311':\n",
    "        act_dict = act_dict_shibuya2223\n",
    "\n",
    "    for folder in folderlist: # å€‹ã€…ã®æ—¥\n",
    "        if not os.path.isdir(os.path.join(base_path, folder)):\n",
    "            continue\n",
    "\n",
    "        filelist = os.listdir(os.path.join(base_path, folder))\n",
    "        \n",
    "        trip_path = os.path.join(base_path, folder, 't_trip.csv')\n",
    "        loc_path = os.path.join(base_path, folder, 't_loc_data.csv')\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—\n",
    "        if not os.path.exists(trip_path) or not os.path.exists(loc_path):\n",
    "            print(f\"ã‚¹ã‚­ãƒƒãƒ—: {folder}ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\")\n",
    "            continue\n",
    "\n",
    "        df_trip = pd.read_csv(trip_path, encoding='shift-jis')\n",
    "        df_loc = pd.read_csv(loc_path, encoding='shift-jis')\n",
    "        df_loc['è¨˜éŒ²æ—¥æ™‚'] = pd.to_datetime(df_loc['è¨˜éŒ²æ—¥æ™‚'].str.split('.').str[0], errors='coerce') # ç§’ã®å°æ•°ç‚¹ä»¥ä¸‹ã‚’åˆ‡ã‚Šæ¨ã¦\n",
    "        df_trip['å‡ºç™ºæ™‚åˆ»'] = pd.to_datetime(df_trip['å‡ºç™ºæ™‚åˆ»'])\n",
    "        df_trip['åˆ°ç€æ™‚åˆ»'] = pd.to_datetime(df_trip['åˆ°ç€æ™‚åˆ»'])\n",
    "        grouped = df_trip.groupby('ãƒ¦ãƒ¼ã‚¶ãƒ¼ID')\n",
    "\n",
    "        #### ãƒ¡ãƒƒã‚·ãƒ¥ã‚³ãƒ¼ãƒ‰ã‚’å–å¾— ####        \n",
    "        gdf_points = gpd.GeoDataFrame(df_loc, geometry=gpd.points_from_xy(df_loc['çµŒåº¦'], df_loc['ç·¯åº¦']), crs=\"EPSG:4326\")\n",
    "        gdf_joined = gpd.sjoin(gdf_points, gdf_mesh, predicate='within')\n",
    "        df_loc2 = df_loc.copy()\n",
    "        df_loc2['mesh'] = gdf_joined['3rdmesh']\n",
    "        \n",
    "        for user_id, group in grouped: # å€‹äººã”ã¨\n",
    "            userid = group['ãƒ¦ãƒ¼ã‚¶ãƒ¼ID'].values[0]\n",
    "            group = group.sort_values(by='å‡ºç™ºæ™‚åˆ»')\n",
    "            acts = list(group['ç›®çš„ã‚³ãƒ¼ãƒ‰ï¼ˆactiveï¼‰']) # ç³»åˆ—\n",
    "            times = list(group['å‡ºç™ºæ™‚åˆ»']) # ç³»åˆ—\n",
    "            \n",
    "            #### ç•°å¸¸å€¤æ’é™¤ #### # ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã«ã‚ˆã£ã¦å‡¦ç†ãŒç•°ãªã‚‹\n",
    "            all_count += 1\n",
    "            if dataset == 'toyosu_2019/201907-202002' or dataset == '99_202110/logs': \n",
    "                if acts[-1] != 300 and acts[-1] != 201:\n",
    "                    if acts[-1] == 500 or acts[-1] == 501 or acts[-1] == 999:\n",
    "                        acts[-1] = 300\n",
    "                        # continue\n",
    "                    # print('kitakushitenai!!!')\n",
    "                if acts[-1] != 300 and acts[-1] != 201:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "                # æœ€åˆã‹ã‚‰å¸°å®…ã®äºº\n",
    "                if acts[0] == 300 or acts[0] == 201:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "\n",
    "            elif dataset == '04_202212old' or dataset == '04_202301new' or dataset == '05_202311':\n",
    "                if acts[-1] != 110:\n",
    "                    if acts[-1] == 998 or acts[-1] == 999:\n",
    "                        acts[-1] = 110\n",
    "                if acts[-1] != 110:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "                if acts[0] == 110:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "\n",
    "            locmesh = []\n",
    "            for i, time in enumerate(times):\n",
    "                time = pd.to_datetime(time, errors='coerce')  # â† ã“ã“ã§å¤‰æ›\n",
    "                user_loc_df = df_loc2[df_loc2['ãƒ¦ãƒ¼ã‚¶ãƒ¼ID'] == userid].copy()\n",
    "\n",
    "                if user_loc_df.empty or pd.isna(time):\n",
    "                    continue\n",
    "                # æ™‚åˆ»å·®ã‚’è¨ˆç®—\n",
    "                user_loc_df['time_diff'] = (user_loc_df['è¨˜éŒ²æ—¥æ™‚'] - time).abs()\n",
    "\n",
    "                # æœ€å°å·®åˆ†ã®è¡Œã‚’å–å¾—ï¼ˆNaTã¯è‡ªå‹•é™¤å¤–ã•ã‚Œã‚‹ï¼‰\n",
    "                nearest_row = user_loc_df.loc[user_loc_df['time_diff'].idxmin()]\n",
    "                mesh = nearest_row['mesh']\n",
    "                #print(f\"User ID: {userid}, Time: {time}, mesh: {mesh}\")\n",
    "                locmesh.append(mesh)\n",
    "            \n",
    "            # timeã¯1æ™‚é–“ãŠãã«ã™ã‚‹\n",
    "            times = list(group['å‡ºç™ºæ™‚åˆ»'].dt.floor('h'))\n",
    "            locmesh.append('<e>')\n",
    "            locmesh.insert(0, '<b>') #OK\n",
    "            acts.insert(0, '<b>')\n",
    "            acts.append('<e>')\n",
    "            times.insert(0, '<b>')\n",
    "            times.append('<e>')\n",
    "\n",
    "            if user_id not in list(user_log_dict.keys()):\n",
    "                user_log_dict[user_id] = len(group)\n",
    "                user_count_dict[user_id] = 1\n",
    "                user_mesh_traj[user_id] = locmesh\n",
    "                user_time_traj[user_id] = times\n",
    "                user_act_traj[user_id] = acts\n",
    "\n",
    "            else:\n",
    "                user_log_dict[user_id] += len(group)\n",
    "                user_count_dict[user_id] += 1\n",
    "                user_mesh_traj[user_id] += locmesh\n",
    "                user_time_traj[user_id] += times\n",
    "                user_act_traj[user_id] += acts\n",
    "\n",
    "    # insertã—ãŸusrer_idãŒindexã«ãªã‚‰ãªã„ã‚ˆã†ã«å‡¦ç†\n",
    "    # df_mesh_traj = pd.DataFrame.from_dict(user_mesh_traj, orient='index')\n",
    "    df_mesh_traj = pd.DataFrame.from_dict(user_mesh_traj, orient='index').reset_index(names='user_id')\n",
    "    # df_time_traj = pd.DataFrame.from_dict(user_time_traj, orient='index')\n",
    "    df_time_traj = pd.DataFrame.from_dict(user_time_traj, orient='index').reset_index(names='user_id')\n",
    "    # df_act_traj = pd.DataFrame.from_dict(user_act_traj, orient='index')\n",
    "    df_act_traj = pd.DataFrame.from_dict(user_act_traj, orient='index').reset_index(names='user_id')\n",
    "\n",
    "    # print('1', df_mesh_traj.head(5))\n",
    "    # print('2test', df_mesh_traj.iloc[0].tolist())\n",
    "\n",
    "    # df_mesh_traj.insert(0, 'user_id', df_mesh_traj.index)\n",
    "    # df_time_traj.insert(0, 'user_id', df_time_traj.index)\n",
    "    # df_act_traj.insert(0, 'user_id', df_act_traj.index)\n",
    "\n",
    "    #### æ´»å‹•ãƒ©ãƒ™ãƒ«ã‚’é›†ç´„ ####\n",
    "    df_act_traj = df_act_traj.applymap(try_convert_int)\n",
    "    df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
    "\n",
    "    # print('1', df_mesh_traj.head(5))\n",
    "    # print('2test', df_mesh_traj.iloc[0].tolist())\n",
    "    # print()\n",
    "    \n",
    "    #### æ´»å‹•å ´æ‰€ã¨ç¨®é¡ãŒé€£ç¶šã—ã¦ã„ã‚‹å ´åˆã¯å‰Šé™¤ ####\n",
    "    # æœ€å¤§åˆ—æ•°ï¼ˆä¸è¶³åˆ†ã‚’åŸ‹ã‚ã‚‹ç”¨ï¼‰\n",
    "    max_len = max(df_act_traj.shape[1], df_time_traj.shape[1], df_mesh_traj.shape[1])\n",
    "\n",
    "    # NaNã‚’åŸ‹ã‚ã‚‹ï¼ˆåˆ—æ•°ã‚’æƒãˆã‚‹ï¼‰\n",
    "    # df_act_traj = df_act_traj.reindex(columns=range(max_len))\n",
    "    # df_time_traj = df_time_traj.reindex(columns=range(max_len))\n",
    "    # df_mesh_traj = df_mesh_traj.reindex(columns=range(max_len))\n",
    "    \n",
    "    # 'user_id' ä»¥å¤–ã®åˆ—ã ã‘ã‚’å¯¾è±¡ã« reindex ã—ã¦ã‹ã‚‰çµåˆ\n",
    "    id_col = df_act_traj[['user_id']]\n",
    "    act_cols = df_act_traj.drop(columns='user_id')\n",
    "    act_cols = act_cols.reindex(columns=range(max_len))\n",
    "    df_act_traj = pd.concat([id_col, act_cols], axis=1)\n",
    "\n",
    "    # åŒæ§˜ã«\n",
    "    id_col = df_time_traj[['user_id']]\n",
    "    time_cols = df_time_traj.drop(columns='user_id')\n",
    "    time_cols = time_cols.reindex(columns=range(max_len))\n",
    "    df_time_traj = pd.concat([id_col, time_cols], axis=1)\n",
    "\n",
    "    id_col = df_mesh_traj[['user_id']]\n",
    "    mesh_cols = df_mesh_traj.drop(columns='user_id')\n",
    "    mesh_cols = mesh_cols.reindex(columns=range(max_len))\n",
    "    df_mesh_traj = pd.concat([id_col, mesh_cols], axis=1)\n",
    "    \n",
    "    # print('2', df_mesh_traj.head(5))\n",
    "    act_cleaned = []\n",
    "    time_cleaned = []\n",
    "    mesh_cleaned = []\n",
    "\n",
    "    # print('2222df_act_traj', df_act_traj.iloc[0].tolist())\n",
    "    # print('222', df_time_traj.columns)\n",
    "    # sys.exit()\n",
    "\n",
    "    for i in range(len(df_act_traj)):\n",
    "        a_row = df_act_traj.iloc[i].tolist()\n",
    "        t_row = df_time_traj.iloc[i].tolist()\n",
    "        m_row = df_mesh_traj.iloc[i].tolist()\n",
    "        a_new, t_new, m_new = remove_consecutive_duplicates(a_row, t_row, m_row)\n",
    "        act_cleaned.append(a_new)\n",
    "        time_cleaned.append(t_new)\n",
    "        mesh_cleaned.append(m_new)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–ã—ã¦ä¿å­˜\n",
    "    df_act_cleaned = pd.DataFrame(act_cleaned)\n",
    "    df_time_cleaned = pd.DataFrame(time_cleaned)\n",
    "    df_mesh_cleaned = pd.DataFrame(mesh_cleaned)\n",
    "    # print('3', df_mesh_cleaned.head(5))\n",
    "    # sys.exit()\n",
    "\n",
    "    df_mesh_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'meshtraj{nameid_list[datasetid]}.csv'), index=False) \n",
    "    df_time_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'timetraj{nameid_list[datasetid]}.csv'), index=False)\n",
    "    df_act_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'acttraj{nameid_list[datasetid]}.csv'), index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce205a",
   "metadata": {},
   "source": [
    "1é€±é–“ã”ã¨ã«åˆ†å‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¤é–¢æ•°ã®ç”¨æ„\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "\n",
    "def split_line_to_segments(line):\n",
    "    \"\"\"\n",
    "    å„è¡Œã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ '<b>'ï¼'<e>' ã§åˆ†å‰²ã—ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’è¿”ã™\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    current = []\n",
    "    for token in line:\n",
    "        if token == '<b>':\n",
    "            current = ['<b>']\n",
    "        elif token == '<e>':\n",
    "            if current:\n",
    "                current.append('<e>')\n",
    "                segments.append(current)\n",
    "        else:\n",
    "            current.append(token)\n",
    "    return segments \n",
    "\n",
    "\n",
    "def flatten_chunks(chunks):\n",
    "    \"\"\"\n",
    "    ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ '<b>' ã¨ '<e>' ä»˜ãã®ãƒ•ãƒ©ãƒƒãƒˆãªè¡Œãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for chunk in chunks:\n",
    "        row = ['<b>']\n",
    "        for seg in chunk:\n",
    "            row.extend(seg)\n",
    "        row.append('<e>')\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def date_of_time_seg(seg):\n",
    "    \"\"\"æ™‚é–“ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º\"\"\"\n",
    "    return datetime.strptime(seg[1], \"%Y-%m-%d %H:%M:%S\").date() # æœ€åˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã ã‘idãŒå…¥ã£ã¦ã‚‹ã®ã§3ç•ªç›®ã®é …ã‹ã‚‰å§‹ã¾ã‚‹\n",
    "\n",
    "\n",
    "def group_into_weeks(segments): #, date_extractor):\n",
    "    \"\"\"\n",
    "    7æ—¥é–“ã”ã¨ã«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "    \"\"\"\n",
    "    if not segments:\n",
    "        return []\n",
    "    chunks = []\n",
    "    current = []\n",
    "    start_date = datetime.strptime(segments[0][2], \"%Y-%m-%d %H:%M:%S\").date() # æœ€åˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã ã‘idãŒå…¥ã£ã¦ã‚‹ã®ã§3ç•ªç›®ã®é …ã‹ã‚‰å§‹ã¾ã‚‹ ok\n",
    "\n",
    "    for i, seg in enumerate(segments):\n",
    "        d = date_of_time_seg(seg)\n",
    "        if d < start_date + timedelta(days=7):\n",
    "            current.append(seg)\n",
    "        else:\n",
    "            chunks.append(current)\n",
    "            current = [seg]\n",
    "            start_date = d\n",
    "        \n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def fill_missing_days(weekly_time_segs, weekly_act_segs, weekly_mesh_segs):\n",
    "    \"\"\"\n",
    "    æ¬ è½æ—¥ã«ã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’æŒ¿å…¥ï¼ˆtime: æ—¥ä»˜ãƒˆãƒ¼ã‚¯ãƒ³ / act: '955'ï¼‰\n",
    "    \"\"\"\n",
    "    # date -> (time_seg, act_seg)\n",
    "    date_map = {}\n",
    "    for t_seg, a_seg, m_seg in zip(weekly_time_segs, weekly_act_segs, weekly_mesh_segs):\n",
    "        date_map[date_of_time_seg(t_seg)] = (t_seg, a_seg, m_seg)\n",
    "\n",
    "    start = date_of_time_seg(weekly_time_segs[0])\n",
    "    filled_time, filled_act, filled_mesh = [], [], []\n",
    "    for i in range(7):\n",
    "        d = start + timedelta(days=i)\n",
    "        if d in date_map:\n",
    "            t_seg, a_seg, m_seg = date_map[d]\n",
    "        else:\n",
    "            t_seg = ['<b>', d.strftime(\"%Y-%m-%d\"), '<e>']\n",
    "            a_seg = ['<b>', \"955\", '<e>']\n",
    "            m_seg = ['<b>', \"999999\", '<e>']\n",
    "\n",
    "        filled_time.append(t_seg)\n",
    "        filled_act.append(a_seg)\n",
    "        filled_mesh.append(m_seg)\n",
    "\n",
    "    return filled_time, filled_act, filled_mesh\n",
    "\n",
    "\n",
    "def write_chunks_to_csv(chunks, output_path):\n",
    "    with open(output_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for segs in chunks:\n",
    "            row = ['<b>'] + [token for seg in segs for token in seg] + ['<e>']\n",
    "            writer.writerow(row[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0f4f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦å®Ÿè¡Œ\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "    act_file = (os.path.join(input_path, f'acttraj{nameid_list[datasetid]}.csv'))\n",
    "    time_file = (os.path.join(input_path, f'timetraj{nameid_list[datasetid]}.csv'))\n",
    "    mesh_file = (os.path.join(input_path, f'meshtraj{nameid_list[datasetid]}.csv'))\n",
    "\n",
    "    out_time_path = os.path.join(input_path, f'time_weekly{nameid_list[datasetid]}.csv')\n",
    "    out_act_path = os.path.join(input_path, f'act_weekly{nameid_list[datasetid]}.csv')\n",
    "    out_mesh_path = os.path.join(input_path, f'mesh_weekly{nameid_list[datasetid]}.csv')\n",
    "    \n",
    "    # csvã‚’èª­ã¿è¾¼ã‚€ï¼šå…ˆé ­ã¯useridãªã®ã§é£›ã°ã™\n",
    "    with open(time_file, newline='') as f:\n",
    "        time_rows = list(csv.reader(f))[1:]\n",
    "    with open(act_file, newline='') as f:\n",
    "        act_rows = list(csv.reader(f))[1:]\n",
    "    with open(mesh_file, newline='') as f:\n",
    "        mesh_rows = list(csv.reader(f))[1:]\n",
    "\n",
    "    all_time_chunks, all_act_chunks, all_mesh_chunks = [], [], []\n",
    "    count = 0 \n",
    "    for t_row, a_row, m_row in zip(time_rows, act_rows, mesh_rows): # å€‹äººã”ã¨\n",
    "        t_segs = split_line_to_segments(t_row) # idã¯æ¶ˆãˆã¦ã‚‹\n",
    "        a_segs = split_line_to_segments(a_row)\n",
    "        m_segs = split_line_to_segments(m_row)\n",
    "\n",
    "        ## ã“ã‚Œã‚ã‚‹ï¼Ÿï¼Ÿ\n",
    "        userid = (t_row[0]) # fixed\n",
    "        weekly_ts = group_into_weeks(t_segs) # æ—¥æ¯ã®b-eã®ãƒªã‚¹ãƒˆ\n",
    "        idx_map = {tuple(seg): idx for idx, seg in enumerate(t_segs)}\n",
    "\n",
    "        for week in weekly_ts:\n",
    "            # å…ƒãƒ‡ãƒ¼ã‚¿ã®actã‚’å¯¾å¿œã¥ã‘\n",
    "            act_week = []\n",
    "            mesh_week = []\n",
    "            for seg in week:\n",
    "                idx = idx_map.get(tuple(seg))\n",
    "                act_week.append(a_segs[idx] if idx is not None else [\"955\"])\n",
    "                mesh_week.append(m_segs[idx] if idx is not None else [\"999999\"])\n",
    "\n",
    "            # æ¬ è½æ—¥ã®è£œå®Œ\n",
    "            filled_t, filled_a, filled_m = fill_missing_days(week, act_week, mesh_week)\n",
    "\n",
    "            filled_t[0].insert(0, f'{userid}')\n",
    "            filled_a[0].insert(0, f'{userid}')\n",
    "            filled_m[0].insert(0, f'{userid}')\n",
    "\n",
    "            all_time_chunks.append(filled_t)\n",
    "            all_act_chunks.append(filled_a)\n",
    "            all_mesh_chunks.append(filled_m)\n",
    "\n",
    "    write_chunks_to_csv(all_time_chunks, out_time_path)\n",
    "    write_chunks_to_csv(all_act_chunks, out_act_path)\n",
    "    write_chunks_to_csv(all_mesh_chunks, out_mesh_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe9e08",
   "metadata": {},
   "source": [
    "é•·ã•ã‚’æƒãˆã¦ã‹ã‚‰ï¼Œ5ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "760c9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§± ç·è¡Œæ•°: 3992, 3992, 3992\n"
     ]
    }
   ],
   "source": [
    "act_all_rows = [] \n",
    "time_all_rows = [] \n",
    "mesh_all_rows = [] \n",
    "\n",
    "act_max_cols = 0\n",
    "time_max_cols = 0\n",
    "mesh_max_cols = 0\n",
    "\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "\n",
    "    time_path = os.path.join(input_path, f'time_weekly{nameid_list[datasetid]}.csv')\n",
    "    act_path = os.path.join(input_path, f'act_weekly{nameid_list[datasetid]}.csv')\n",
    "    mesh_path = os.path.join(input_path, f'mesh_weekly{nameid_list[datasetid]}.csv')\n",
    "\n",
    "    # èª­ã¿è¾¼ã¿ + ç©ºæ–‡å­—ãƒ»Noneãƒ»\"NaN\"é™¤å»\n",
    "    with open(act_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        act_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "    \n",
    "    with open(time_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        time_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "\n",
    "    with open(mesh_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        mesh_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "    \n",
    "    # æœ€å¤§åˆ—æ•°ã‚’æ›´æ–°\n",
    "    for row in act_cleaned_rows:\n",
    "        act_max_cols = max(act_max_cols, len(row))\n",
    "    \n",
    "    for row in time_cleaned_rows:\n",
    "        time_max_cols = max(time_max_cols, len(row))\n",
    "\n",
    "    for row in mesh_cleaned_rows:\n",
    "        mesh_max_cols = max(mesh_max_cols, len(row))\n",
    "    \n",
    "    act_all_rows.extend(act_cleaned_rows)\n",
    "    time_all_rows.extend(time_cleaned_rows)\n",
    "    mesh_all_rows.extend(mesh_cleaned_rows)\n",
    "\n",
    "# è¡Œã®é•·ã•ã‚’æœ€å¤§åˆ—æ•°ã«ãã‚ãˆã€<p>ã§åŸ‹ã‚ã‚‹\n",
    "act_padded_rows = [row + ['<p>'] * (act_max_cols - len(row)) for row in act_all_rows]\n",
    "time_padded_rows = [row + ['<p>'] * (time_max_cols - len(row)) for row in time_all_rows]\n",
    "mesh_padded_rows = [row + ['<p>'] * (mesh_max_cols - len(row)) for row in mesh_all_rows]\n",
    "\n",
    "# çµåˆçµæœã‚’ä¿å­˜\n",
    "act_output_path = os.path.join(input_path, 'act_weekly_filled.csv')\n",
    "time_output_path = os.path.join(input_path, 'time_weekly_filled.csv')\n",
    "mesh_output_path = os.path.join(input_path, 'mesh_weekly_filled.csv')\n",
    "\n",
    "with open(act_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(act_padded_rows)\n",
    "\n",
    "with open(time_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(time_padded_rows)\n",
    "\n",
    "with open(mesh_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(mesh_padded_rows)\n",
    "\n",
    "print(f\"ğŸ§± ç·è¡Œæ•°: {len(act_padded_rows)}, {len(time_padded_rows)}, {len(mesh_padded_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f1e77a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1425381383.py:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_time = df_time.applymap(lambda x: convert_datetime_to_hour(x) if pd.notna(x) else x)\n"
     ]
    }
   ],
   "source": [
    "def convert_datetime_to_hour(token):\n",
    "    try:\n",
    "        # æ™‚åˆ»ãŒå«ã¾ã‚Œã‚‹å ´åˆï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä»˜ãæ—¥ä»˜ï¼‰\n",
    "        dt = pd.to_datetime(token, errors='raise')\n",
    "        if dt.hour == 0 and \"00:00:00\" not in token:\n",
    "            return 25 # token  # ãŸã¨ãˆã° \"2022-12-21\" ã®ã‚ˆã†ãªå ´åˆ\n",
    "        return str(dt.hour)\n",
    "    except Exception:\n",
    "        # å¤‰æ›ã§ããªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™\n",
    "        return token\n",
    "## \n",
    "df_time = pd.read_csv(os.path.join(input_path, 'time_weekly_filled.csv'), header=None)\n",
    "df_time = df_time.applymap(lambda x: convert_datetime_to_hour(x) if pd.notna(x) else x)\n",
    "df_time.to_csv(os.path.join(input_path, 'time_weekly_filled_hour.csv'), index=False, header=False)\n",
    "# converted_row = [convert_datetime_to_hour(tok) for tok in row]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fcf11",
   "metadata": {},
   "source": [
    "æ—¥ä»˜å¤‰æ›´æ™‚ç‚¹ã‚’Sãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0f8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act = pd.read_csv(os.path.join(input_path, 'act_weekly_filled.csv'), header=None)\n",
    "df_time = pd.read_csv(os.path.join(input_path, 'time_weekly_filled_hour.csv'), header=None)\n",
    "df_mesh = pd.read_csv(os.path.join(input_path, 'mesh_weekly_filled.csv'), header=None)\n",
    "\n",
    "# å„è¡Œè¦‹ã¦ã„ã£ã¦ï¼Œ<b>ã‚’æ¶ˆã™ï¼†<e>ã‚’<S>ã«ç½®æ›\n",
    "df_act = df_act.replace('<e>', '<s>')\n",
    "df_time = df_time.replace('<e>', '<s>')\n",
    "df_mesh = df_mesh.replace('<e>', '<s>')\n",
    "\n",
    "# ç‰¹å®šã®æ–‡å­—ã‚’å‰Šé™¤ã—ã¦å·¦è©°ã‚ï¼ˆæ–‡å­—åˆ—çµåˆã§å†æ§‹æˆï¼‰\n",
    "def remove_and_shift(row, char='<b>'):\n",
    "    # å„ã‚»ãƒ«ã‹ã‚‰ç‰¹å®šæ–‡å­—ã‚’å‰Šé™¤ã—ã€è¡Œã‚’å†æ§‹æˆ\n",
    "    cleaned = [str(cell).replace(char, '') for cell in row]\n",
    "    # ç©ºã§ãªã„ã‚‚ã®ã‚’å·¦ã«å¯„ã›ã€ç©ºæ–‡å­—ã‚’å³ã«è©°ã‚ã‚‹\n",
    "    non_empty = [c for c in cleaned if c]\n",
    "    empty = [''] * (len(row) - len(non_empty))\n",
    "    return pd.Series(non_empty + empty)\n",
    "\n",
    "# å„è¡Œã«é©ç”¨\n",
    "df_act_cleaned = df_act.apply(remove_and_shift, axis=1)\n",
    "df_time_cleaned = df_time.apply(remove_and_shift, axis=1)\n",
    "df_mesh_cleaned = df_mesh.apply(remove_and_shift, axis=1)\n",
    "\n",
    "df_act_cleaned = df_act_cleaned.fillna('<p>')\n",
    "df_time_cleaned = df_time_cleaned.fillna('<p>')\n",
    "df_mesh_cleaned = df_mesh_cleaned.fillna('<p>')\n",
    "# print(df_act_cleaned)\n",
    "\n",
    "df_act_cleaned = df_act_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "df_time_cleaned = df_time_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "df_mesh_cleaned = df_mesh_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "\n",
    "df_act = df_act_cleaned\n",
    "df_time = df_time_cleaned\n",
    "df_mesh = df_mesh_cleaned\n",
    "\n",
    "#### ã“ã“ã‹ã‚‰ <s> ã‚’ <s1>, <s2>, ... ã«å¤‰æ› ####\n",
    "def convert_d(row):\n",
    "    # row ã”ã¨ã«ã‚«ã‚¦ãƒ³ãƒˆå¤‰ãˆã‚‹ã®ã§\n",
    "    count = 0\n",
    "    for i in range(len(row)):\n",
    "        if row[i] == '<s>':\n",
    "            count += 1\n",
    "            if count <= 6: # countãŒ7, 8ã®æ™‚ã¯ä¸€ç•ªå¾Œã‚ã®ã‚„ã¤ãªã®ã§ç„¡è¦–ã™ã‚‹\n",
    "                row[i] = '<s' + str(count) + '>'\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return row\n",
    "\n",
    "for i in range(len(df_time)):\n",
    "    # 1è¡Œãšã¤å–ã‚Šå‡ºã™\n",
    "    row = df_time.iloc[i]\n",
    "    # 1è¡Œã®ä¸­ã«<s>ãŒã„ãã¤ã‚ã‚‹ã‹ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    row = convert_d(row)\n",
    "    # å¤‰æ›ã—ãŸè¡Œã‚’å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æˆ»ã™\n",
    "    df_time.iloc[i] = row\n",
    "\n",
    "for i in range(len(df_act)):\n",
    "    # 1è¡Œãšã¤å–ã‚Šå‡ºã™\n",
    "    row = df_act.iloc[i]\n",
    "    # 1è¡Œã®ä¸­ã«<s>ãŒã„ãã¤ã‚ã‚‹ã‹ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    row = convert_d(row)\n",
    "    # å¤‰æ›ã—ãŸè¡Œã‚’å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æˆ»ã™\n",
    "    df_act.iloc[i] = row\n",
    "\n",
    "for i in range(len(df_mesh)):\n",
    "    # 1è¡Œãšã¤å–ã‚Šå‡ºã™\n",
    "    row = df_mesh.iloc[i]\n",
    "    # 1è¡Œã®ä¸­ã«<s>ãŒã„ãã¤ã‚ã‚‹ã‹ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    row = convert_d(row)\n",
    "    # å¤‰æ›ã—ãŸè¡Œã‚’å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æˆ»ã™\n",
    "    df_mesh.iloc[i] = row\n",
    "\n",
    "df_time.to_csv(os.path.join(input_path, 'time_weekly_filled_structured.csv'), index=False)\n",
    "df_act.to_csv(os.path.join(input_path, 'act_weekly_filled_structured.csv'), index=False)\n",
    "df_mesh.to_csv(os.path.join(input_path, 'mesh_weekly_filled_structured.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5ef52",
   "metadata": {},
   "source": [
    "ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªmeshã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6dbef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0       1       2       3       4       5       6       7       8  \\\n",
      "0     23002  533946  533946  533946  533946  533946  533946    <s1>  533946   \n",
      "1     23002  533946  533946  533946  533946  533935  533946  533946    <s1>   \n",
      "2     23003  533945  533945    <s1>  999999    <s2>  999999    <s3>  533933   \n",
      "3     23003  533934  533935    <s1>  999999    <s2>  999999    <s3>  999999   \n",
      "4     23003  533933  533935    <s1>  999999    <s2>  999999    <s3>  999999   \n",
      "...     ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "3987  21402  533934  533914  533945    <s1>  999999    <s2>  999999    <s3>   \n",
      "3988  21402  533946  533945    <s1>  999999    <s2>  999999    <s3>  999999   \n",
      "3989  21415    <s1>  999999    <s2>  999999    <s3>  999999    <s4>  999999   \n",
      "3990  21457  533945  533935  533945  533945    <s1>  999999    <s2>  999999   \n",
      "3991  21550  533946  533946  533946  533946    <s1>  999999    <s2>  999999   \n",
      "\n",
      "           9  ...   78   79   80   81   82   83   84   85   86   87  \n",
      "0     533946  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "1     533946  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "2     533935  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3       <s4>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "4       <s4>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "3987  999999  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3988    <s4>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3989    <s5>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3990    <s3>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3991    <s3>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "\n",
      "[3992 rows x 88 columns]\n"
     ]
    }
   ],
   "source": [
    "df_mesh = pd.read_csv(os.path.join(input_path, 'mesh_weekly_filled_structured.csv'))\n",
    "print(df_mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b3ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['523910' '523930' '523936' '523947' '523950' '523956' '523960' '523965'\n",
      " '523966' '523967' '523971' '523972' '523973' '523974' '523975' '523976'\n",
      " '533901' '533902' '533903' '533904' '533905' '533907' '533911' '533912'\n",
      " '533913' '533914' '533915' '533916' '533917' '533922' '533923' '533924'\n",
      " '533925' '533926' '533930' '533931' '533932' '533933' '533934' '533935'\n",
      " '533936' '533937' '533941' '533942' '533943' '533944' '533945' '533946'\n",
      " '533947' '533950' '533951' '533952' '533953' '533954' '533955' '533956'\n",
      " '533957' '533961' '533962' '533963' '533964' '533965' '533966' '533967'\n",
      " '533970' '533973' '533974' '533975' '533976' '533977' '534001' '534003'\n",
      " '534010' '534011' '534012' '534020' '534021' '534022' '534023' '534030'\n",
      " '534031' '534032' '534040' '534041' '534042' '534045' '534050' '534051'\n",
      " '534052' '534053' '534060' '534063' '534065' '534071' '543902' '543903'\n",
      " '543904' '543905' '543906' '543912' '543913' '543914' '543915' '543922'\n",
      " '543923' '543924' '543925' '543930' '543931' '543932' '543933' '543934'\n",
      " '543940' '543942' '543943' '543945' '543952' '543960' '543966' '543967'\n",
      " '543976' '544000' '544001' '544003' '544010' '544011' '544012' '544020'\n",
      " '544032' '544042' '544043' '544044' '544050' '544064' '544065' '544071'\n",
      " '544075' '999999' '<p>' '<s1>' '<s2>' '<s3>' '<s4>' '<s5>' '<s6>' '<s>']\n",
      "unique mesh num: 146\n"
     ]
    }
   ],
   "source": [
    "df_mesh_val = df_mesh.iloc[:, 1:].values\n",
    "\n",
    "nunique_mesh = np.unique(df_mesh_val)\n",
    "print(nunique_mesh)\n",
    "print('unique mesh num:', len(nunique_mesh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8edee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   KEY_CODE MESH1_ID MESH2_ID MESH3_ID  OBJ_ID  \\\n",
      "0  54390000     5439       00       00       1   \n",
      "1  54390001     5439       00       01       2   \n",
      "2  54390002     5439       00       02       3   \n",
      "3  54390003     5439       00       03       4   \n",
      "4  54390004     5439       00       04       5   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((139.0125 36, 139 36, 139 36.00833, 1...  \n",
      "1  POLYGON ((139.025 36, 139.0125 36, 139.0125 36...  \n",
      "2  POLYGON ((139.0375 36, 139.025 36, 139.025 36....  \n",
      "3  POLYGON ((139.05 36, 139.0375 36, 139.0375 36....  \n",
      "4  POLYGON ((139.0625 36, 139.05 36, 139.05 36.00...  \n"
     ]
    }
   ],
   "source": [
    "gdf_mesh5 = gpd.read_file(os.path.join(base_path0, '3rdmesh5439/MESH05439.shp'))\n",
    "print(gdf_mesh5.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1995de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1.0\n",
      "1       3.0\n",
      "2       2.0\n",
      "3       3.0\n",
      "4       2.0\n",
      "       ... \n",
      "3988    3.0\n",
      "3989    2.0\n",
      "3990    2.0\n",
      "3991    3.0\n",
      "3992    2.0\n",
      "Name: 1, Length: 3993, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_act = pd.read_csv(os.path.join(input_path, 'act_weekly_filled_structured.csv'), header=None)\n",
    "print(df_act.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c116beab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1.0\n",
      "1       3.0\n",
      "2       2.0\n",
      "3       3.0\n",
      "4       2.0\n",
      "       ... \n",
      "3988    3.0\n",
      "3989    2.0\n",
      "3990    2.0\n",
      "3991    3.0\n",
      "3992    2.0\n",
      "Name: 1, Length: 3993, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_act.iloc[:, 1] = df_act.iloc[:, 1].astype(int) # [:-2]\n",
    "\n",
    "print(df_act.iloc[:, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "177eb28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2\n",
      "1       5\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "3988    2\n",
      "3989    1\n",
      "3990    1\n",
      "3991    3\n",
      "3992    5\n",
      "Name: 2, Length: 3993, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_act.iloc[:, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31ab61d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1.0\n",
      "1       3.0\n",
      "2       2.0\n",
      "3       3.0\n",
      "4       2.0\n",
      "       ... \n",
      "3988    3.0\n",
      "3989    2.0\n",
      "3990    2.0\n",
      "3991    3.0\n",
      "3992    2.0\n",
      "Name: 1, Length: 3993, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def try_convert_int(x):\n",
    "    try:\n",
    "        f = float(x)\n",
    "        i = int(f)\n",
    "        return i if f == i else x  # æ•´æ•°ã ã£ãŸã‚‰intã«ã™ã‚‹ã€ãã†ã§ãªã‘ã‚Œã°ãã®ã¾ã¾\n",
    "    except:\n",
    "        return x  \n",
    "    \n",
    "df_act.iloc[:, 1] = df_act.iloc[:, 1].map(try_convert_int)\n",
    "\n",
    "print(df_act.iloc[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bb2bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       3\n",
      "2       2\n",
      "3       3\n",
      "4       2\n",
      "       ..\n",
      "3988    3\n",
      "3989    2\n",
      "3990    2\n",
      "3991    3\n",
      "3992    2\n",
      "Name: 1, Length: 3993, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_38253/1952329140.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0       1\n",
      "1       3\n",
      "2       2\n",
      "3       3\n",
      "4       2\n",
      "       ..\n",
      "3988    3\n",
      "3989    2\n",
      "3990    2\n",
      "3991    3\n",
      "3992    2\n",
      "Name: 1, Length: 3993, dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_act.iloc[:, 1] = df_act.iloc[:, 1].astype(str).str.split('.').str[0]\n"
     ]
    }
   ],
   "source": [
    "df_act.iloc[:, 1] = df_act.iloc[:, 1].astype(str).str.split('.').str[0]\n",
    "print(df_act.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "768b9345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0  1  2     3     4     5     6     7     8     9  ...   78   79  \\\n",
      "0     23002  3  5     2     3     2     1  <s1>     3     2  ...  <p>  <p>   \n",
      "1     23002  2  1     3     2     2     2     1  <s1>     3  ...  <p>  <p>   \n",
      "2     23003  3  1  <s1>   955  <s2>   955  <s3>     3     1  ...  <p>  <p>   \n",
      "3     23003  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "4     23003  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "...     ... .. ..   ...   ...   ...   ...   ...   ...   ...  ...  ...  ...   \n",
      "3987  21402  3  2     1  <s1>   955  <s2>   955  <s3>   955  ...  <p>  <p>   \n",
      "3988  21402  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "3989  21415  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "3990  21457  3  3     5     1  <s1>   955  <s2>   955  <s3>  ...  <p>  <p>   \n",
      "3991  21550  2  5     2     1  <s1>   955  <s2>   955  <s3>  ...  <p>  <p>   \n",
      "\n",
      "       80   81   82   83   84   85   86   87  \n",
      "0     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "1     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "2     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "4     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "3987  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3988  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3989  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3990  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3991  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "\n",
      "[3992 rows x 88 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_38253/3325456784.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0       3\n",
      "1       2\n",
      "2       3\n",
      "3       2\n",
      "4       2\n",
      "       ..\n",
      "3987    3\n",
      "3988    2\n",
      "3989    2\n",
      "3990    3\n",
      "3991    2\n",
      "Name: 1, Length: 3992, dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_act.iloc[:, 1] = df_act.iloc[:, 1].astype(str).str.split('.').str[0] # æœ€åˆã®è¡Œå‹•ã‚’æ•´æ•°ã«\n"
     ]
    }
   ],
   "source": [
    "df_act = pd.read_csv(os.path.join(input_path, 'act_weekly_filled_structured.csv')) #, header=None)\n",
    "df_time = pd.read_csv(os.path.join(input_path, 'time_weekly_filled_structured.csv'))#, header=None)\n",
    "df_act.iloc[:, 1] = df_act.iloc[:, 1].astype(str).str.split('.').str[0] # æœ€åˆã®è¡Œå‹•ã‚’æ•´æ•°ã«\n",
    "\n",
    "# print(df_act.iloc[:, 1])\n",
    "print(df_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b47ee30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act num: 19\n",
      "['1' '1.0' '2' '2.0' '3' '3.0' '4' '4.0' '5' '5.0' '955' '<p>' '<s1>'\n",
      " '<s2>' '<s3>' '<s4>' '<s5>' '<s6>' '<s>']\n"
     ]
    }
   ],
   "source": [
    "df_act_val = df_act.iloc[1:, 1:].values.astype(str)\n",
    "nunique_act = np.unique(df_act_val)\n",
    "# actn = len(np.unique(df_act.iloc[:, 1:].values))\n",
    "print('act num:', len(nunique_act)) \n",
    "print(nunique_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53b20a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0  1  2     3     4     5     6     7     8     9  ...   78   79  \\\n",
      "0     23002  3  5     2     3     2     1  <s1>     3     2  ...  <p>  <p>   \n",
      "1     23002  2  1     3     2     2     2     1  <s1>     3  ...  <p>  <p>   \n",
      "2     23003  3  1  <s1>   955  <s2>   955  <s3>     3     1  ...  <p>  <p>   \n",
      "3     23003  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "4     23003  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "...     ... .. ..   ...   ...   ...   ...   ...   ...   ...  ...  ...  ...   \n",
      "3987  21402  3  2     1  <s1>   955  <s2>   955  <s3>   955  ...  <p>  <p>   \n",
      "3988  21402  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "3989  21415  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "3990  21457  3  3     5     1  <s1>   955  <s2>   955  <s3>  ...  <p>  <p>   \n",
      "3991  21550  2  5     2     1  <s1>   955  <s2>   955  <s3>  ...  <p>  <p>   \n",
      "\n",
      "       80   81   82   83   84   85   86   87  \n",
      "0     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "1     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "2     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "4     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "3987  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3988  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3989  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3990  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3991  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "\n",
      "[3992 rows x 88 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_38253/1871717435.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act.iloc[:, 1:] = df_act.iloc[:, 1:].applymap(to_int_if_possible)\n"
     ]
    }
   ],
   "source": [
    "print(df_act)\n",
    "def to_int_if_possible(x):\n",
    "    try:\n",
    "        f = float(x)\n",
    "        i = int(f)\n",
    "        return i if f == i else x  # 2.0 â†’ 2, 2.5 ã¯ãã®ã¾ã¾ï¼ˆå¿…è¦ãªã‚‰åˆ‡ã‚Šæ¨ã¦ã«ã—ã¦ã‚‚å¯ï¼‰\n",
    "    except:\n",
    "        return x  # å¤‰æ›ã§ããªã„æ–‡å­—åˆ—ï¼ˆä¾‹ï¼š'<s1>'ãªã©ï¼‰ã¯ãã®ã¾ã¾è¿”ã™\n",
    "\n",
    "df_act.iloc[:, 1:] = df_act.iloc[:, 1:].applymap(to_int_if_possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09ed6aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act num: 14\n",
      "['1' '2' '3' '4' '5' '955' '<p>' '<s1>' '<s2>' '<s3>' '<s4>' '<s5>' '<s6>'\n",
      " '<s>']\n"
     ]
    }
   ],
   "source": [
    "df_act_val = df_act.iloc[1:, 1:].values.astype(str)\n",
    "nunique_act = np.unique(df_act_val)\n",
    "# actn = len(np.unique(df_act.iloc[:, 1:].values))\n",
    "print('act num:', len(nunique_act)) \n",
    "print(nunique_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "272b4274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time num: 33\n",
      "['0' '1' '10' '11' '12' '13' '14' '15' '16' '17' '18' '19' '2' '20' '21'\n",
      " '22' '23' '25' '3' '4' '5' '6' '7' '8' '9' '<p>' '<s1>' '<s2>' '<s3>'\n",
      " '<s4>' '<s5>' '<s6>' '<s>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_time_val = df_time.iloc[1:, 1:].values.astype(str)\n",
    "nunique_time = np.unique(df_time_val)\n",
    "# actn = len(np.unique(df_act.iloc[:, 1:].values))\n",
    "print('time num:', len(nunique_time)) \n",
    "print(nunique_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a434a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
