{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da038142",
   "metadata": {},
   "source": [
    " PPからlocationシーケンスの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e521eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import sys\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b72ad8",
   "metadata": {},
   "source": [
    "time, mesh, actの個人ごとのトータルシーケンスを作成（この後に1週間ごとにデータ分割）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c20a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip dataの出発時刻に併せて loc_dataからlon, latを取り出す\n",
    "base_path0 = '/Users/matsunagatakahiro/Desktop/jrres/PPcameraTG/gpslog'#/04_202212old'\n",
    "input_path = '/Users/matsunagatakahiro/Desktop/res2025/ActFormer/RoutesFormer/actgpt0512/input'\n",
    "dataset_list = ['04_202212old', '04_202301new', '05_202311', 'toyosu_2019/201907-202002', '99_202110/logs']\n",
    "nameid_list = ['01', '02', '03', 'toyosu', 'shibu21']\n",
    "gdf_mesh1 = gpd.read_file(os.path.join(base_path0, '3rdmesh5339/MESH05339.shp'))\n",
    "gdf_mesh2 = gpd.read_file(os.path.join(base_path0, '3rdmesh5340/MESH05340.shp'))\n",
    "gdf_mesh3 = gpd.read_file(os.path.join(base_path0, '3rdmesh5440/MESH05440.shp'))\n",
    "gdf_mesh4 = gpd.read_file(os.path.join(base_path0, '3rdmesh5239/MESH05239.shp'))\n",
    "gdf_mesh5 = gpd.read_file(os.path.join(base_path0, '3rdmesh5439/MESH05439.shp'))\n",
    "\n",
    "gdf_mesh = pd.concat([gdf_mesh1, gdf_mesh2, gdf_mesh3, gdf_mesh4, gdf_mesh5], ignore_index=True)\n",
    "gdf_mesh = gdf_mesh.to_crs(\"EPSG:4326\") # WGS84に変換\n",
    "gdf_mesh['3rdmesh'] = gdf_mesh['KEY_CODE'].astype(str).str[0:6] # 3次メッシュコード\n",
    "\n",
    "act_dict_shibuya2223 = {\n",
    "    110: 1, # H\n",
    "    200: 2, # S\n",
    "    210: 2, # S\n",
    "    220: 1, # hotel->home\n",
    "    300: 3, # W \n",
    "    310: 4, # W2\n",
    "    400: 3, # W\n",
    "    998: 5, # Other\n",
    "    999: 5 # , # Other\n",
    "}\n",
    "\n",
    "act_dict_shibuya21toyosu = {\n",
    "    300: 1, # H\n",
    "    201: 1, # H\n",
    "    400: 2, # S\n",
    "    401: 2, \n",
    "    402: 2, \n",
    "    404: 2, \n",
    "    405: 2, \n",
    "    406: 2, \n",
    "    100: 3, # W\n",
    "    200: 4, # W2\n",
    "    403: 5, # Ohter\n",
    "    407: 5, # Ohter\n",
    "    500: 5, # Ohter\n",
    "    501: 5, # Ohter\n",
    "    999: 5 #, # Ohter`\n",
    "}\n",
    "\n",
    "def remove_consecutive_duplicates(act_row, time_row, mesh_row):\n",
    "    # print('remove_consecutive_duplicates', act_row, time_row, mesh_row)\n",
    "    new_act = [act_row[0]]\n",
    "    new_time = [time_row[0]]\n",
    "    new_mesh = [mesh_row[0]]\n",
    "    for i in range(1, len(act_row)):\n",
    "        if not (act_row[i] == act_row[i-1] and mesh_row[i] == mesh_row[i-1]):\n",
    "            new_act.append(act_row[i])\n",
    "            new_time.append(time_row[i])\n",
    "            new_mesh.append(mesh_row[i])\n",
    "    return pd.Series(new_act), pd.Series(new_time), pd.Series(new_mesh)\n",
    "\n",
    "def try_convert_int(x):\n",
    "    if x in ['<b>', '<e>']:\n",
    "        return x\n",
    "    try:\n",
    "        return int(float(x))  # 300.0 → 300 などにも対応\n",
    "    except:\n",
    "        return x  # 変換できなければそのまま\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfe4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スキップ: 10004_20190915（ファイルが存在しない）\n",
      "スキップ: 10004_20190916（ファイルが存在しない）\n",
      "スキップ: 10004_20191020（ファイルが存在しない）\n",
      "スキップ: 10004_20191229（ファイルが存在しない）\n",
      "スキップ: 10004_20191230（ファイルが存在しない）\n",
      "スキップ: 10004_20191231（ファイルが存在しない）\n",
      "スキップ: 10004_20200101（ファイルが存在しない）\n",
      "スキップ: 10004_20200102（ファイルが存在しない）\n",
      "スキップ: 10004_20200103（ファイルが存在しない）\n",
      "スキップ: 10004_20200104（ファイルが存在しない）\n",
      "スキップ: 10004_20200105（ファイルが存在しない）\n",
      "スキップ: 10004_20200111（ファイルが存在しない）\n",
      "スキップ: 10004_20200112（ファイルが存在しない）\n",
      "スキップ: 10004_20200113（ファイルが存在しない）\n",
      "スキップ: 10004_20200126（ファイルが存在しない）\n",
      "スキップ: 10004_20200127（ファイルが存在しない）\n",
      "スキップ: 10004_20200201（ファイルが存在しない）\n",
      "スキップ: 10004_20200202（ファイルが存在しない）\n",
      "スキップ: 10004_20200208（ファイルが存在しない）\n",
      "スキップ: 10004_20200209（ファイルが存在しない）\n",
      "スキップ: 10004_20200210（ファイルが存在しない）\n",
      "スキップ: 10004_20200211（ファイルが存在しない）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "    user_log_dict = {}\n",
    "    user_count_dict = {} # 何日登場したかのカウント\n",
    "    user_mesh_traj = {}\n",
    "    user_time_traj = {}\n",
    "    user_act_traj = {}\n",
    "    nokitaku_count = 0\n",
    "    all_count = 0\n",
    "    folderlist = sorted(folderlist)\n",
    "\n",
    "    if dataset == 'toyosu_2019/201907-202002' or dataset == '99_202110/logs': \n",
    "        act_dict = act_dict_shibuya21toyosu\n",
    "    elif dataset == '04_202212old' or dataset == '04_202301new' or dataset == '05_202311':\n",
    "        act_dict = act_dict_shibuya2223\n",
    "\n",
    "    for folder in folderlist: # 個々の日\n",
    "        if not os.path.isdir(os.path.join(base_path, folder)):\n",
    "            continue\n",
    "\n",
    "        filelist = os.listdir(os.path.join(base_path, folder))\n",
    "        \n",
    "        trip_path = os.path.join(base_path, folder, 't_trip.csv')\n",
    "        loc_path = os.path.join(base_path, folder, 't_loc_data.csv')\n",
    "        # ファイルがなければスキップ\n",
    "        if not os.path.exists(trip_path) or not os.path.exists(loc_path):\n",
    "            print(f\"スキップ: {folder}（ファイルが存在しない）\")\n",
    "            continue\n",
    "\n",
    "        df_trip = pd.read_csv(trip_path, encoding='shift-jis')\n",
    "        df_loc = pd.read_csv(loc_path, encoding='shift-jis')\n",
    "        df_loc['記録日時'] = pd.to_datetime(df_loc['記録日時'].str.split('.').str[0], errors='coerce') # 秒の小数点以下を切り捨て\n",
    "        df_trip['出発時刻'] = pd.to_datetime(df_trip['出発時刻'])\n",
    "        df_trip['到着時刻'] = pd.to_datetime(df_trip['到着時刻'])\n",
    "        grouped = df_trip.groupby('ユーザーID')\n",
    "\n",
    "        #### メッシュコードを取得 ####        \n",
    "        gdf_points = gpd.GeoDataFrame(df_loc, geometry=gpd.points_from_xy(df_loc['経度'], df_loc['緯度']), crs=\"EPSG:4326\")\n",
    "        gdf_joined = gpd.sjoin(gdf_points, gdf_mesh, predicate='within')\n",
    "        df_loc2 = df_loc.copy()\n",
    "        df_loc2['mesh'] = gdf_joined['3rdmesh']\n",
    "        \n",
    "        for user_id, group in grouped: # 個人ごと\n",
    "            userid = group['ユーザーID'].values[0]\n",
    "            group = group.sort_values(by='出発時刻')\n",
    "            acts = list(group['目的コード（active）']) # 系列\n",
    "            times = list(group['出発時刻']) # 系列\n",
    "            \n",
    "            #### 異常値排除 #### # タイミングによって処理が異なる\n",
    "            all_count += 1\n",
    "            if dataset == 'toyosu_2019/201907-202002' or dataset == '99_202110/logs': \n",
    "                if acts[-1] != 300 and acts[-1] != 201:\n",
    "                    if acts[-1] == 500 or acts[-1] == 501 or acts[-1] == 999:\n",
    "                        acts[-1] = 300\n",
    "                        # continue\n",
    "                    # print('kitakushitenai!!!')\n",
    "                if acts[-1] != 300 and acts[-1] != 201:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "                # 最初から帰宅の人\n",
    "                if acts[0] == 300 or acts[0] == 201:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "\n",
    "            elif dataset == '04_202212old' or dataset == '04_202301new' or dataset == '05_202311':\n",
    "                if acts[-1] != 110:\n",
    "                    if acts[-1] == 998 or acts[-1] == 999:\n",
    "                        acts[-1] = 110\n",
    "                if acts[-1] != 110:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "                if acts[0] == 110:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "\n",
    "            locmesh = []\n",
    "            for i, time in enumerate(times):\n",
    "                time = pd.to_datetime(time, errors='coerce')  # ← ここで変換\n",
    "                user_loc_df = df_loc2[df_loc2['ユーザーID'] == userid].copy()\n",
    "\n",
    "                if user_loc_df.empty or pd.isna(time):\n",
    "                    continue\n",
    "                # 時刻差を計算\n",
    "                user_loc_df['time_diff'] = (user_loc_df['記録日時'] - time).abs()\n",
    "\n",
    "                # 最小差分の行を取得（NaTは自動除外される）\n",
    "                nearest_row = user_loc_df.loc[user_loc_df['time_diff'].idxmin()]\n",
    "                mesh = nearest_row['mesh']\n",
    "                #print(f\"User ID: {userid}, Time: {time}, mesh: {mesh}\")\n",
    "                locmesh.append(mesh)\n",
    "            \n",
    "            # timeは1時間おきにする\n",
    "            times = list(group['出発時刻'].dt.floor('h'))\n",
    "            locmesh.append('<e>')\n",
    "            locmesh.insert(0, '<b>') #OK\n",
    "            acts.insert(0, '<b>')\n",
    "            acts.append('<e>')\n",
    "            times.insert(0, '<b>')\n",
    "            times.append('<e>')\n",
    "\n",
    "            if user_id not in list(user_log_dict.keys()):\n",
    "                user_log_dict[user_id] = len(group)\n",
    "                user_count_dict[user_id] = 1\n",
    "                user_mesh_traj[user_id] = locmesh\n",
    "                user_time_traj[user_id] = times\n",
    "                user_act_traj[user_id] = acts\n",
    "\n",
    "            else:\n",
    "                user_log_dict[user_id] += len(group)\n",
    "                user_count_dict[user_id] += 1\n",
    "                user_mesh_traj[user_id] += locmesh\n",
    "                user_time_traj[user_id] += times\n",
    "                user_act_traj[user_id] += acts\n",
    "\n",
    "    # insertしたusrer_idがindexにならないように処理\n",
    "    # df_mesh_traj = pd.DataFrame.from_dict(user_mesh_traj, orient='index')\n",
    "    df_mesh_traj = pd.DataFrame.from_dict(user_mesh_traj, orient='index').reset_index(names='user_id')\n",
    "    # df_time_traj = pd.DataFrame.from_dict(user_time_traj, orient='index')\n",
    "    df_time_traj = pd.DataFrame.from_dict(user_time_traj, orient='index').reset_index(names='user_id')\n",
    "    # df_act_traj = pd.DataFrame.from_dict(user_act_traj, orient='index')\n",
    "    df_act_traj = pd.DataFrame.from_dict(user_act_traj, orient='index').reset_index(names='user_id')\n",
    "\n",
    "    # print('1', df_mesh_traj.head(5))\n",
    "    # print('2test', df_mesh_traj.iloc[0].tolist())\n",
    "\n",
    "    # df_mesh_traj.insert(0, 'user_id', df_mesh_traj.index)\n",
    "    # df_time_traj.insert(0, 'user_id', df_time_traj.index)\n",
    "    # df_act_traj.insert(0, 'user_id', df_act_traj.index)\n",
    "\n",
    "    #### 活動ラベルを集約 ####\n",
    "    df_act_traj = df_act_traj.applymap(try_convert_int)\n",
    "    df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
    "\n",
    "    # print('1', df_mesh_traj.head(5))\n",
    "    # print('2test', df_mesh_traj.iloc[0].tolist())\n",
    "    # print()\n",
    "    \n",
    "    #### 活動場所と種類が連続している場合は削除 ####\n",
    "    # 最大列数（不足分を埋める用）\n",
    "    max_len = max(df_act_traj.shape[1], df_time_traj.shape[1], df_mesh_traj.shape[1])\n",
    "\n",
    "    # NaNを埋める（列数を揃える）\n",
    "    # df_act_traj = df_act_traj.reindex(columns=range(max_len))\n",
    "    # df_time_traj = df_time_traj.reindex(columns=range(max_len))\n",
    "    # df_mesh_traj = df_mesh_traj.reindex(columns=range(max_len))\n",
    "    \n",
    "    # 'user_id' 以外の列だけを対象に reindex してから結合\n",
    "    id_col = df_act_traj[['user_id']]\n",
    "    act_cols = df_act_traj.drop(columns='user_id')\n",
    "    act_cols = act_cols.reindex(columns=range(max_len))\n",
    "    df_act_traj = pd.concat([id_col, act_cols], axis=1)\n",
    "\n",
    "    # 同様に\n",
    "    id_col = df_time_traj[['user_id']]\n",
    "    time_cols = df_time_traj.drop(columns='user_id')\n",
    "    time_cols = time_cols.reindex(columns=range(max_len))\n",
    "    df_time_traj = pd.concat([id_col, time_cols], axis=1)\n",
    "\n",
    "    id_col = df_mesh_traj[['user_id']]\n",
    "    mesh_cols = df_mesh_traj.drop(columns='user_id')\n",
    "    mesh_cols = mesh_cols.reindex(columns=range(max_len))\n",
    "    df_mesh_traj = pd.concat([id_col, mesh_cols], axis=1)\n",
    "    \n",
    "    # print('2', df_mesh_traj.head(5))\n",
    "    act_cleaned = []\n",
    "    time_cleaned = []\n",
    "    mesh_cleaned = []\n",
    "\n",
    "    # print('2222df_act_traj', df_act_traj.iloc[0].tolist())\n",
    "    # print('222', df_time_traj.columns)\n",
    "    # sys.exit()\n",
    "\n",
    "    for i in range(len(df_act_traj)):\n",
    "        a_row = df_act_traj.iloc[i].tolist()\n",
    "        t_row = df_time_traj.iloc[i].tolist()\n",
    "        m_row = df_mesh_traj.iloc[i].tolist()\n",
    "        a_new, t_new, m_new = remove_consecutive_duplicates(a_row, t_row, m_row)\n",
    "        act_cleaned.append(a_new)\n",
    "        time_cleaned.append(t_new)\n",
    "        mesh_cleaned.append(m_new)\n",
    "\n",
    "    # データフレーム化して保存\n",
    "    df_act_cleaned = pd.DataFrame(act_cleaned)\n",
    "    df_time_cleaned = pd.DataFrame(time_cleaned)\n",
    "    df_mesh_cleaned = pd.DataFrame(mesh_cleaned)\n",
    "    # print('3', df_mesh_cleaned.head(5))\n",
    "    # sys.exit()\n",
    "\n",
    "    df_mesh_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'meshtraj{nameid_list[datasetid]}.csv'), index=False) \n",
    "    df_time_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'timetraj{nameid_list[datasetid]}.csv'), index=False)\n",
    "    df_act_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'acttraj{nameid_list[datasetid]}.csv'), index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce205a",
   "metadata": {},
   "source": [
    "1週間ごとに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基礎関数の用意\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "\n",
    "def split_line_to_segments(line):\n",
    "    \"\"\"\n",
    "    各行のトークン列を '<b>'／'<e>' で分割し、セグメントリストを返す\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    current = []\n",
    "    for token in line:\n",
    "        if token == '<b>':\n",
    "            current = ['<b>']\n",
    "        elif token == '<e>':\n",
    "            if current:\n",
    "                current.append('<e>')\n",
    "                segments.append(current)\n",
    "        else:\n",
    "            current.append(token)\n",
    "    return segments \n",
    "\n",
    "\n",
    "def flatten_chunks(chunks):\n",
    "    \"\"\"\n",
    "    チャンク化されたセグメントを '<b>' と '<e>' 付きのフラットな行リストに変換\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for chunk in chunks:\n",
    "        row = ['<b>']\n",
    "        for seg in chunk:\n",
    "            row.extend(seg)\n",
    "        row.append('<e>')\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def date_of_time_seg(seg):\n",
    "    \"\"\"時間セグメントから日付を抽出\"\"\"\n",
    "    return datetime.strptime(seg[1], \"%Y-%m-%d %H:%M:%S\").date() # 最初のセグメントだけidが入ってるので3番目の項から始まる\n",
    "\n",
    "\n",
    "def group_into_weeks(segments): #, date_extractor):\n",
    "    \"\"\"\n",
    "    7日間ごとにセグメントをグループ化\n",
    "    \"\"\"\n",
    "    if not segments:\n",
    "        return []\n",
    "    chunks = []\n",
    "    current = []\n",
    "    start_date = datetime.strptime(segments[0][2], \"%Y-%m-%d %H:%M:%S\").date() # 最初のセグメントだけidが入ってるので3番目の項から始まる ok\n",
    "\n",
    "    for i, seg in enumerate(segments):\n",
    "        d = date_of_time_seg(seg)\n",
    "        if d < start_date + timedelta(days=7):\n",
    "            current.append(seg)\n",
    "        else:\n",
    "            chunks.append(current)\n",
    "            current = [seg]\n",
    "            start_date = d\n",
    "        \n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def fill_missing_days(weekly_time_segs, weekly_act_segs, weekly_mesh_segs):\n",
    "    \"\"\"\n",
    "    欠落日にはプレースホルダーを挿入（time: 日付トークン / act: '955'）\n",
    "    \"\"\"\n",
    "    # date -> (time_seg, act_seg)\n",
    "    date_map = {}\n",
    "    for t_seg, a_seg, m_seg in zip(weekly_time_segs, weekly_act_segs, weekly_mesh_segs):\n",
    "        date_map[date_of_time_seg(t_seg)] = (t_seg, a_seg, m_seg)\n",
    "\n",
    "    start = date_of_time_seg(weekly_time_segs[0])\n",
    "    filled_time, filled_act, filled_mesh = [], [], []\n",
    "    for i in range(7):\n",
    "        d = start + timedelta(days=i)\n",
    "        if d in date_map:\n",
    "            t_seg, a_seg, m_seg = date_map[d]\n",
    "        else:\n",
    "            t_seg = ['<b>', d.strftime(\"%Y-%m-%d\"), '<e>']\n",
    "            a_seg = ['<b>', \"955\", '<e>']\n",
    "            m_seg = ['<b>', \"999999\", '<e>']\n",
    "\n",
    "        filled_time.append(t_seg)\n",
    "        filled_act.append(a_seg)\n",
    "        filled_mesh.append(m_seg)\n",
    "\n",
    "    return filled_time, filled_act, filled_mesh\n",
    "\n",
    "\n",
    "def write_chunks_to_csv(chunks, output_path):\n",
    "    with open(output_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for segs in chunks:\n",
    "            row = ['<b>'] + [token for seg in segs for token in seg] + ['<e>']\n",
    "            writer.writerow(row[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0f4f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データセットに対して実行\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "    act_file = (os.path.join(input_path, f'acttraj{nameid_list[datasetid]}.csv'))\n",
    "    time_file = (os.path.join(input_path, f'timetraj{nameid_list[datasetid]}.csv'))\n",
    "    mesh_file = (os.path.join(input_path, f'meshtraj{nameid_list[datasetid]}.csv'))\n",
    "\n",
    "    out_time_path = os.path.join(input_path, f'time_weekly{nameid_list[datasetid]}.csv')\n",
    "    out_act_path = os.path.join(input_path, f'act_weekly{nameid_list[datasetid]}.csv')\n",
    "    out_mesh_path = os.path.join(input_path, f'mesh_weekly{nameid_list[datasetid]}.csv')\n",
    "    \n",
    "    # csvを読み込む：先頭はuseridなので飛ばす\n",
    "    with open(time_file, newline='') as f:\n",
    "        time_rows = list(csv.reader(f))[1:]\n",
    "    with open(act_file, newline='') as f:\n",
    "        act_rows = list(csv.reader(f))[1:]\n",
    "    with open(mesh_file, newline='') as f:\n",
    "        mesh_rows = list(csv.reader(f))[1:]\n",
    "\n",
    "    all_time_chunks, all_act_chunks, all_mesh_chunks = [], [], []\n",
    "    count = 0 \n",
    "    for t_row, a_row, m_row in zip(time_rows, act_rows, mesh_rows): # 個人ごと\n",
    "        t_segs = split_line_to_segments(t_row) # idは消えてる\n",
    "        a_segs = split_line_to_segments(a_row)\n",
    "        m_segs = split_line_to_segments(m_row)\n",
    "\n",
    "        ## これある？？\n",
    "        userid = (t_row[0]) # fixed\n",
    "        weekly_ts = group_into_weeks(t_segs) # 日毎のb-eのリスト\n",
    "        idx_map = {tuple(seg): idx for idx, seg in enumerate(t_segs)}\n",
    "\n",
    "        for week in weekly_ts:\n",
    "            # 元データのactを対応づけ\n",
    "            act_week = []\n",
    "            mesh_week = []\n",
    "            for seg in week:\n",
    "                idx = idx_map.get(tuple(seg))\n",
    "                act_week.append(a_segs[idx] if idx is not None else [\"955\"])\n",
    "                mesh_week.append(m_segs[idx] if idx is not None else [\"999999\"])\n",
    "\n",
    "            # 欠落日の補完\n",
    "            filled_t, filled_a, filled_m = fill_missing_days(week, act_week, mesh_week)\n",
    "\n",
    "            filled_t[0].insert(0, f'{userid}')\n",
    "            filled_a[0].insert(0, f'{userid}')\n",
    "            filled_m[0].insert(0, f'{userid}')\n",
    "\n",
    "            all_time_chunks.append(filled_t)\n",
    "            all_act_chunks.append(filled_a)\n",
    "            all_mesh_chunks.append(filled_m)\n",
    "\n",
    "    write_chunks_to_csv(all_time_chunks, out_time_path)\n",
    "    write_chunks_to_csv(all_act_chunks, out_act_path)\n",
    "    write_chunks_to_csv(all_mesh_chunks, out_mesh_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe9e08",
   "metadata": {},
   "source": [
    "長さを揃えてから，5つのデータを統合する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "760c9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧱 総行数: 3992, 3992, 3992\n"
     ]
    }
   ],
   "source": [
    "act_all_rows = [] \n",
    "time_all_rows = [] \n",
    "mesh_all_rows = [] \n",
    "\n",
    "act_max_cols = 0\n",
    "time_max_cols = 0\n",
    "mesh_max_cols = 0\n",
    "\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "\n",
    "    time_path = os.path.join(input_path, f'time_weekly{nameid_list[datasetid]}.csv')\n",
    "    act_path = os.path.join(input_path, f'act_weekly{nameid_list[datasetid]}.csv')\n",
    "    mesh_path = os.path.join(input_path, f'mesh_weekly{nameid_list[datasetid]}.csv')\n",
    "\n",
    "    # 読み込み + 空文字・None・\"NaN\"除去\n",
    "    with open(act_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        act_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "    \n",
    "    with open(time_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        time_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "\n",
    "    with open(mesh_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        mesh_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "    \n",
    "    # 最大列数を更新\n",
    "    for row in act_cleaned_rows:\n",
    "        act_max_cols = max(act_max_cols, len(row))\n",
    "    \n",
    "    for row in time_cleaned_rows:\n",
    "        time_max_cols = max(time_max_cols, len(row))\n",
    "\n",
    "    for row in mesh_cleaned_rows:\n",
    "        mesh_max_cols = max(mesh_max_cols, len(row))\n",
    "    \n",
    "    act_all_rows.extend(act_cleaned_rows)\n",
    "    time_all_rows.extend(time_cleaned_rows)\n",
    "    mesh_all_rows.extend(mesh_cleaned_rows)\n",
    "\n",
    "# 行の長さを最大列数にそろえ、<p>で埋める\n",
    "act_padded_rows = [row + ['<p>'] * (act_max_cols - len(row)) for row in act_all_rows]\n",
    "time_padded_rows = [row + ['<p>'] * (time_max_cols - len(row)) for row in time_all_rows]\n",
    "mesh_padded_rows = [row + ['<p>'] * (mesh_max_cols - len(row)) for row in mesh_all_rows]\n",
    "\n",
    "# 結合結果を保存\n",
    "act_output_path = os.path.join(input_path, 'act_weekly_filled.csv')\n",
    "time_output_path = os.path.join(input_path, 'time_weekly_filled.csv')\n",
    "mesh_output_path = os.path.join(input_path, 'mesh_weekly_filled.csv')\n",
    "\n",
    "with open(act_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(act_padded_rows)\n",
    "\n",
    "with open(time_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(time_padded_rows)\n",
    "\n",
    "with open(mesh_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(mesh_padded_rows)\n",
    "\n",
    "print(f\"🧱 総行数: {len(act_padded_rows)}, {len(time_padded_rows)}, {len(mesh_padded_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f1e77a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1425381383.py:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_time = df_time.applymap(lambda x: convert_datetime_to_hour(x) if pd.notna(x) else x)\n"
     ]
    }
   ],
   "source": [
    "def convert_datetime_to_hour(token):\n",
    "    try:\n",
    "        # 時刻が含まれる場合（フォーマット付き日付）\n",
    "        dt = pd.to_datetime(token, errors='raise')\n",
    "        if dt.hour == 0 and \"00:00:00\" not in token:\n",
    "            return 25 # token  # たとえば \"2022-12-21\" のような場合\n",
    "        return str(dt.hour)\n",
    "    except Exception:\n",
    "        # 変換できない場合はそのまま返す\n",
    "        return token\n",
    "## \n",
    "df_time = pd.read_csv(os.path.join(input_path, 'time_weekly_filled.csv'), header=None)\n",
    "df_time = df_time.applymap(lambda x: convert_datetime_to_hour(x) if pd.notna(x) else x)\n",
    "df_time.to_csv(os.path.join(input_path, 'time_weekly_filled_hour.csv'), index=False, header=False)\n",
    "# converted_row = [convert_datetime_to_hour(tok) for tok in row]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fcf11",
   "metadata": {},
   "source": [
    "日付変更時点をSトークンに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0f8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act = pd.read_csv(os.path.join(input_path, 'act_weekly_filled.csv'), header=None)\n",
    "df_time = pd.read_csv(os.path.join(input_path, 'time_weekly_filled_hour.csv'), header=None)\n",
    "df_mesh = pd.read_csv(os.path.join(input_path, 'mesh_weekly_filled.csv'), header=None)\n",
    "\n",
    "# 各行見ていって，<b>を消す＆<e>を<S>に置換\n",
    "df_act = df_act.replace('<e>', '<s>')\n",
    "df_time = df_time.replace('<e>', '<s>')\n",
    "df_mesh = df_mesh.replace('<e>', '<s>')\n",
    "\n",
    "# 特定の文字を削除して左詰め（文字列結合で再構成）\n",
    "def remove_and_shift(row, char='<b>'):\n",
    "    # 各セルから特定文字を削除し、行を再構成\n",
    "    cleaned = [str(cell).replace(char, '') for cell in row]\n",
    "    # 空でないものを左に寄せ、空文字を右に詰める\n",
    "    non_empty = [c for c in cleaned if c]\n",
    "    empty = [''] * (len(row) - len(non_empty))\n",
    "    return pd.Series(non_empty + empty)\n",
    "\n",
    "# 各行に適用\n",
    "df_act_cleaned = df_act.apply(remove_and_shift, axis=1)\n",
    "df_time_cleaned = df_time.apply(remove_and_shift, axis=1)\n",
    "df_mesh_cleaned = df_mesh.apply(remove_and_shift, axis=1)\n",
    "\n",
    "df_act_cleaned = df_act_cleaned.fillna('<p>')\n",
    "df_time_cleaned = df_time_cleaned.fillna('<p>')\n",
    "df_mesh_cleaned = df_mesh_cleaned.fillna('<p>')\n",
    "# print(df_act_cleaned)\n",
    "\n",
    "df_act_cleaned = df_act_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "df_time_cleaned = df_time_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "df_mesh_cleaned = df_mesh_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "\n",
    "df_act = df_act_cleaned\n",
    "df_time = df_time_cleaned\n",
    "df_mesh = df_mesh_cleaned\n",
    "\n",
    "#### ここから <s> を <s1>, <s2>, ... に変換 ####\n",
    "def convert_d(row):\n",
    "    # row ごとにカウント変えるので\n",
    "    count = 0\n",
    "    for i in range(len(row)):\n",
    "        if row[i] == '<s>':\n",
    "            count += 1\n",
    "            if count <= 6: # countが7, 8の時は一番後ろのやつなので無視する\n",
    "                row[i] = '<s' + str(count) + '>'\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return row\n",
    "\n",
    "for i in range(len(df_time)):\n",
    "    # 1行ずつ取り出す\n",
    "    row = df_time.iloc[i]\n",
    "    # 1行の中に<s>がいくつあるかをカウント\n",
    "    row = convert_d(row)\n",
    "    # 変換した行を元のデータフレームに戻す\n",
    "    df_time.iloc[i] = row\n",
    "\n",
    "for i in range(len(df_act)):\n",
    "    # 1行ずつ取り出す\n",
    "    row = df_act.iloc[i]\n",
    "    # 1行の中に<s>がいくつあるかをカウント\n",
    "    row = convert_d(row)\n",
    "    # 変換した行を元のデータフレームに戻す\n",
    "    df_act.iloc[i] = row\n",
    "\n",
    "for i in range(len(df_mesh)):\n",
    "    # 1行ずつ取り出す\n",
    "    row = df_mesh.iloc[i]\n",
    "    # 1行の中に<s>がいくつあるかをカウント\n",
    "    row = convert_d(row)\n",
    "    # 変換した行を元のデータフレームに戻す\n",
    "    df_mesh.iloc[i] = row\n",
    "\n",
    "df_time.to_csv(os.path.join(input_path, 'time_weekly_filled_structured.csv'), index=False)\n",
    "df_act.to_csv(os.path.join(input_path, 'act_weekly_filled_structured.csv'), index=False)\n",
    "df_mesh.to_csv(os.path.join(input_path, 'mesh_weekly_filled_structured.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5ef52",
   "metadata": {},
   "source": [
    "ユニークなmeshの数をカウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6dbef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0       1       2       3       4       5       6       7       8  \\\n",
      "0     23002  533946  533946  533946  533946  533946  533946    <s1>  533946   \n",
      "1     23002  533946  533946  533946  533946  533935  533946  533946    <s1>   \n",
      "2     23003  533945  533945    <s1>  999999    <s2>  999999    <s3>  533933   \n",
      "3     23003  533934  533935    <s1>  999999    <s2>  999999    <s3>  999999   \n",
      "4     23003  533933  533935    <s1>  999999    <s2>  999999    <s3>  999999   \n",
      "...     ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "3987  21402  533934  533914  533945    <s1>  999999    <s2>  999999    <s3>   \n",
      "3988  21402  533946  533945    <s1>  999999    <s2>  999999    <s3>  999999   \n",
      "3989  21415    <s1>  999999    <s2>  999999    <s3>  999999    <s4>  999999   \n",
      "3990  21457  533945  533935  533945  533945    <s1>  999999    <s2>  999999   \n",
      "3991  21550  533946  533946  533946  533946    <s1>  999999    <s2>  999999   \n",
      "\n",
      "           9  ...   78   79   80   81   82   83   84   85   86   87  \n",
      "0     533946  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "1     533946  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "2     533935  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3       <s4>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "4       <s4>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "3987  999999  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3988    <s4>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3989    <s5>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3990    <s3>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3991    <s3>  ...  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "\n",
      "[3992 rows x 88 columns]\n"
     ]
    }
   ],
   "source": [
    "df_mesh = pd.read_csv(os.path.join(input_path, 'mesh_weekly_filled_structured.csv'))\n",
    "print(df_mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b3ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['523910' '523930' '523936' '523947' '523950' '523956' '523960' '523965'\n",
      " '523966' '523967' '523971' '523972' '523973' '523974' '523975' '523976'\n",
      " '533901' '533902' '533903' '533904' '533905' '533907' '533911' '533912'\n",
      " '533913' '533914' '533915' '533916' '533917' '533922' '533923' '533924'\n",
      " '533925' '533926' '533930' '533931' '533932' '533933' '533934' '533935'\n",
      " '533936' '533937' '533941' '533942' '533943' '533944' '533945' '533946'\n",
      " '533947' '533950' '533951' '533952' '533953' '533954' '533955' '533956'\n",
      " '533957' '533961' '533962' '533963' '533964' '533965' '533966' '533967'\n",
      " '533970' '533973' '533974' '533975' '533976' '533977' '534001' '534003'\n",
      " '534010' '534011' '534012' '534020' '534021' '534022' '534023' '534030'\n",
      " '534031' '534032' '534040' '534041' '534042' '534045' '534050' '534051'\n",
      " '534052' '534053' '534060' '534063' '534065' '534071' '543902' '543903'\n",
      " '543904' '543905' '543906' '543912' '543913' '543914' '543915' '543922'\n",
      " '543923' '543924' '543925' '543930' '543931' '543932' '543933' '543934'\n",
      " '543940' '543942' '543943' '543945' '543952' '543960' '543966' '543967'\n",
      " '543976' '544000' '544001' '544003' '544010' '544011' '544012' '544020'\n",
      " '544032' '544042' '544043' '544044' '544050' '544064' '544065' '544071'\n",
      " '544075' '999999' '<p>' '<s1>' '<s2>' '<s3>' '<s4>' '<s5>' '<s6>' '<s>']\n",
      "unique mesh num: 146\n"
     ]
    }
   ],
   "source": [
    "df_mesh_val = df_mesh.iloc[:, 1:].values\n",
    "\n",
    "nunique_mesh = np.unique(df_mesh_val)\n",
    "print(nunique_mesh)\n",
    "print('unique mesh num:', len(nunique_mesh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8edee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   KEY_CODE MESH1_ID MESH2_ID MESH3_ID  OBJ_ID  \\\n",
      "0  54390000     5439       00       00       1   \n",
      "1  54390001     5439       00       01       2   \n",
      "2  54390002     5439       00       02       3   \n",
      "3  54390003     5439       00       03       4   \n",
      "4  54390004     5439       00       04       5   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((139.0125 36, 139 36, 139 36.00833, 1...  \n",
      "1  POLYGON ((139.025 36, 139.0125 36, 139.0125 36...  \n",
      "2  POLYGON ((139.0375 36, 139.025 36, 139.025 36....  \n",
      "3  POLYGON ((139.05 36, 139.0375 36, 139.0375 36....  \n",
      "4  POLYGON ((139.0625 36, 139.05 36, 139.05 36.00...  \n"
     ]
    }
   ],
   "source": [
    "gdf_mesh5 = gpd.read_file(os.path.join(base_path0, '3rdmesh5439/MESH05439.shp'))\n",
    "print(gdf_mesh5.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1995de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1.0\n",
      "1       3.0\n",
      "2       2.0\n",
      "3       3.0\n",
      "4       2.0\n",
      "       ... \n",
      "3988    3.0\n",
      "3989    2.0\n",
      "3990    2.0\n",
      "3991    3.0\n",
      "3992    2.0\n",
      "Name: 1, Length: 3993, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_act = pd.read_csv(os.path.join(input_path, 'act_weekly_filled_structured.csv'), header=None)\n",
    "print(df_act.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c116beab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1.0\n",
      "1       3.0\n",
      "2       2.0\n",
      "3       3.0\n",
      "4       2.0\n",
      "       ... \n",
      "3988    3.0\n",
      "3989    2.0\n",
      "3990    2.0\n",
      "3991    3.0\n",
      "3992    2.0\n",
      "Name: 1, Length: 3993, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_act.iloc[:, 1] = df_act.iloc[:, 1].astype(int) # [:-2]\n",
    "\n",
    "print(df_act.iloc[:, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "177eb28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2\n",
      "1       5\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "3988    2\n",
      "3989    1\n",
      "3990    1\n",
      "3991    3\n",
      "3992    5\n",
      "Name: 2, Length: 3993, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_act.iloc[:, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31ab61d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1.0\n",
      "1       3.0\n",
      "2       2.0\n",
      "3       3.0\n",
      "4       2.0\n",
      "       ... \n",
      "3988    3.0\n",
      "3989    2.0\n",
      "3990    2.0\n",
      "3991    3.0\n",
      "3992    2.0\n",
      "Name: 1, Length: 3993, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def try_convert_int(x):\n",
    "    try:\n",
    "        f = float(x)\n",
    "        i = int(f)\n",
    "        return i if f == i else x  # 整数だったらintにする、そうでなければそのまま\n",
    "    except:\n",
    "        return x  \n",
    "    \n",
    "df_act.iloc[:, 1] = df_act.iloc[:, 1].map(try_convert_int)\n",
    "\n",
    "print(df_act.iloc[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bb2bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       3\n",
      "2       2\n",
      "3       3\n",
      "4       2\n",
      "       ..\n",
      "3988    3\n",
      "3989    2\n",
      "3990    2\n",
      "3991    3\n",
      "3992    2\n",
      "Name: 1, Length: 3993, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_38253/1952329140.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0       1\n",
      "1       3\n",
      "2       2\n",
      "3       3\n",
      "4       2\n",
      "       ..\n",
      "3988    3\n",
      "3989    2\n",
      "3990    2\n",
      "3991    3\n",
      "3992    2\n",
      "Name: 1, Length: 3993, dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_act.iloc[:, 1] = df_act.iloc[:, 1].astype(str).str.split('.').str[0]\n"
     ]
    }
   ],
   "source": [
    "df_act.iloc[:, 1] = df_act.iloc[:, 1].astype(str).str.split('.').str[0]\n",
    "print(df_act.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "768b9345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0  1  2     3     4     5     6     7     8     9  ...   78   79  \\\n",
      "0     23002  3  5     2     3     2     1  <s1>     3     2  ...  <p>  <p>   \n",
      "1     23002  2  1     3     2     2     2     1  <s1>     3  ...  <p>  <p>   \n",
      "2     23003  3  1  <s1>   955  <s2>   955  <s3>     3     1  ...  <p>  <p>   \n",
      "3     23003  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "4     23003  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "...     ... .. ..   ...   ...   ...   ...   ...   ...   ...  ...  ...  ...   \n",
      "3987  21402  3  2     1  <s1>   955  <s2>   955  <s3>   955  ...  <p>  <p>   \n",
      "3988  21402  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "3989  21415  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "3990  21457  3  3     5     1  <s1>   955  <s2>   955  <s3>  ...  <p>  <p>   \n",
      "3991  21550  2  5     2     1  <s1>   955  <s2>   955  <s3>  ...  <p>  <p>   \n",
      "\n",
      "       80   81   82   83   84   85   86   87  \n",
      "0     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "1     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "2     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "4     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "3987  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3988  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3989  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3990  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3991  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "\n",
      "[3992 rows x 88 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_38253/3325456784.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0       3\n",
      "1       2\n",
      "2       3\n",
      "3       2\n",
      "4       2\n",
      "       ..\n",
      "3987    3\n",
      "3988    2\n",
      "3989    2\n",
      "3990    3\n",
      "3991    2\n",
      "Name: 1, Length: 3992, dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_act.iloc[:, 1] = df_act.iloc[:, 1].astype(str).str.split('.').str[0] # 最初の行動を整数に\n"
     ]
    }
   ],
   "source": [
    "df_act = pd.read_csv(os.path.join(input_path, 'act_weekly_filled_structured.csv')) #, header=None)\n",
    "df_time = pd.read_csv(os.path.join(input_path, 'time_weekly_filled_structured.csv'))#, header=None)\n",
    "df_act.iloc[:, 1] = df_act.iloc[:, 1].astype(str).str.split('.').str[0] # 最初の行動を整数に\n",
    "\n",
    "# print(df_act.iloc[:, 1])\n",
    "print(df_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b47ee30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act num: 19\n",
      "['1' '1.0' '2' '2.0' '3' '3.0' '4' '4.0' '5' '5.0' '955' '<p>' '<s1>'\n",
      " '<s2>' '<s3>' '<s4>' '<s5>' '<s6>' '<s>']\n"
     ]
    }
   ],
   "source": [
    "df_act_val = df_act.iloc[1:, 1:].values.astype(str)\n",
    "nunique_act = np.unique(df_act_val)\n",
    "# actn = len(np.unique(df_act.iloc[:, 1:].values))\n",
    "print('act num:', len(nunique_act)) \n",
    "print(nunique_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53b20a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0  1  2     3     4     5     6     7     8     9  ...   78   79  \\\n",
      "0     23002  3  5     2     3     2     1  <s1>     3     2  ...  <p>  <p>   \n",
      "1     23002  2  1     3     2     2     2     1  <s1>     3  ...  <p>  <p>   \n",
      "2     23003  3  1  <s1>   955  <s2>   955  <s3>     3     1  ...  <p>  <p>   \n",
      "3     23003  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "4     23003  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "...     ... .. ..   ...   ...   ...   ...   ...   ...   ...  ...  ...  ...   \n",
      "3987  21402  3  2     1  <s1>   955  <s2>   955  <s3>   955  ...  <p>  <p>   \n",
      "3988  21402  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "3989  21415  2  1  <s1>   955  <s2>   955  <s3>   955  <s4>  ...  <p>  <p>   \n",
      "3990  21457  3  3     5     1  <s1>   955  <s2>   955  <s3>  ...  <p>  <p>   \n",
      "3991  21550  2  5     2     1  <s1>   955  <s2>   955  <s3>  ...  <p>  <p>   \n",
      "\n",
      "       80   81   82   83   84   85   86   87  \n",
      "0     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "1     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "2     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "4     <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "3987  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3988  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3989  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3990  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "3991  <p>  <p>  <p>  <p>  <p>  <p>  <p>  <p>  \n",
      "\n",
      "[3992 rows x 88 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_38253/1871717435.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act.iloc[:, 1:] = df_act.iloc[:, 1:].applymap(to_int_if_possible)\n"
     ]
    }
   ],
   "source": [
    "print(df_act)\n",
    "def to_int_if_possible(x):\n",
    "    try:\n",
    "        f = float(x)\n",
    "        i = int(f)\n",
    "        return i if f == i else x  # 2.0 → 2, 2.5 はそのまま（必要なら切り捨てにしても可）\n",
    "    except:\n",
    "        return x  # 変換できない文字列（例：'<s1>'など）はそのまま返す\n",
    "\n",
    "df_act.iloc[:, 1:] = df_act.iloc[:, 1:].applymap(to_int_if_possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09ed6aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act num: 14\n",
      "['1' '2' '3' '4' '5' '955' '<p>' '<s1>' '<s2>' '<s3>' '<s4>' '<s5>' '<s6>'\n",
      " '<s>']\n"
     ]
    }
   ],
   "source": [
    "df_act_val = df_act.iloc[1:, 1:].values.astype(str)\n",
    "nunique_act = np.unique(df_act_val)\n",
    "# actn = len(np.unique(df_act.iloc[:, 1:].values))\n",
    "print('act num:', len(nunique_act)) \n",
    "print(nunique_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "272b4274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time num: 33\n",
      "['0' '1' '10' '11' '12' '13' '14' '15' '16' '17' '18' '19' '2' '20' '21'\n",
      " '22' '23' '25' '3' '4' '5' '6' '7' '8' '9' '<p>' '<s1>' '<s2>' '<s3>'\n",
      " '<s4>' '<s5>' '<s6>' '<s>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_time_val = df_time.iloc[1:, 1:].values.astype(str)\n",
    "nunique_time = np.unique(df_time_val)\n",
    "# actn = len(np.unique(df_act.iloc[:, 1:].values))\n",
    "print('time num:', len(nunique_time)) \n",
    "print(nunique_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a434a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
