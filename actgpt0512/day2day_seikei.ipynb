{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da038142",
   "metadata": {},
   "source": [
    " PPã‹ã‚‰locationã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e521eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import sys\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b72ad8",
   "metadata": {},
   "source": [
    "time, mesh, actã®å€‹äººã”ã¨ã®ãƒˆãƒ¼ã‚¿ãƒ«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆã“ã®å¾Œã«1é€±é–“ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c20a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip dataã®å‡ºç™ºæ™‚åˆ»ã«ä½µã›ã¦ loc_dataã‹ã‚‰lon, latã‚’å–ã‚Šå‡ºã™\n",
    "base_path0 = '/Users/matsunagatakahiro/Desktop/jrres/PPcameraTG/gpslog'#/04_202212old'\n",
    "input_path = '/Users/matsunagatakahiro/Desktop/res2025/ActFormer/RoutesFormer/actgpt0512/input'\n",
    "dataset_list = ['04_202212old', '04_202301new', '05_202311', 'toyosu_2019/201907-202002', '99_202110/logs']\n",
    "nameid_list = ['01', '02', '03', 'toyosu', 'shibu21']\n",
    "gdf_mesh1 = gpd.read_file(os.path.join(base_path0, '3rdmesh5339/MESH05339.shp'))\n",
    "gdf_mesh2 = gpd.read_file(os.path.join(base_path0, '3rdmesh5340/MESH05340.shp'))\n",
    "gdf_mesh3 = gpd.read_file(os.path.join(base_path0, '3rdmesh5440/MESH05440.shp'))\n",
    "gdf_mesh4 = gpd.read_file(os.path.join(base_path0, '3rdmesh5239/MESH05239.shp'))\n",
    "gdf_mesh5 = gpd.read_file(os.path.join(base_path0, '3rdmesh5439/MESH05439.shp'))\n",
    "\n",
    "gdf_mesh = pd.concat([gdf_mesh1, gdf_mesh2, gdf_mesh3, gdf_mesh4, gdf_mesh5], ignore_index=True)\n",
    "gdf_mesh = gdf_mesh.to_crs(\"EPSG:4326\") # WGS84ã«å¤‰æ›\n",
    "gdf_mesh['3rdmesh'] = gdf_mesh['KEY_CODE'].astype(str).str[0:6] # 3æ¬¡ãƒ¡ãƒƒã‚·ãƒ¥ã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "act_dict_shibuya2223 = {\n",
    "    110: 1, # H\n",
    "    200: 2, # S\n",
    "    210: 2, # S\n",
    "    220: 1, # hotel->home\n",
    "    300: 3, # W \n",
    "    310: 4, # W2\n",
    "    400: 3, # W\n",
    "    998: 5, # Other\n",
    "    999: 5 # , # Other\n",
    "}\n",
    "\n",
    "act_dict_shibuya21toyosu = {\n",
    "    300: 1, # H\n",
    "    201: 1, # H\n",
    "    400: 2, # S\n",
    "    401: 2, \n",
    "    402: 2, \n",
    "    404: 2, \n",
    "    405: 2, \n",
    "    406: 2, \n",
    "    100: 3, # W\n",
    "    200: 4, # W2\n",
    "    403: 5, # Ohter\n",
    "    407: 5, # Ohter\n",
    "    500: 5, # Ohter\n",
    "    501: 5, # Ohter\n",
    "    999: 5 #, # Ohter`\n",
    "}\n",
    "\n",
    "def remove_consecutive_duplicates(act_row, time_row, mesh_row):\n",
    "    # print('remove_consecutive_duplicates', act_row, time_row, mesh_row)\n",
    "    new_act = [act_row[0]]\n",
    "    new_time = [time_row[0]]\n",
    "    new_mesh = [mesh_row[0]]\n",
    "    for i in range(1, len(act_row)):\n",
    "        if not (act_row[i] == act_row[i-1] and mesh_row[i] == mesh_row[i-1]):\n",
    "            new_act.append(act_row[i])\n",
    "            new_time.append(time_row[i])\n",
    "            new_mesh.append(mesh_row[i])\n",
    "    return pd.Series(new_act), pd.Series(new_time), pd.Series(new_mesh)\n",
    "\n",
    "def try_convert_int(x):\n",
    "    if x in ['<b>', '<e>']:\n",
    "        return x\n",
    "    try:\n",
    "        return int(float(x))  # 300.0 â†’ 300 ãªã©ã«ã‚‚å¯¾å¿œ\n",
    "    except:\n",
    "        return x  # å¤‰æ›ã§ããªã‘ã‚Œã°ãã®ã¾ã¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfe4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20190915ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20190916ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20191020ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20191229ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20191230ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20191231ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200101ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200102ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200103ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200104ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200105ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200111ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200112ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200113ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200126ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200127ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200201ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200202ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200208ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200209ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200210ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n",
      "ã‚¹ã‚­ãƒƒãƒ—: 10004_20200211ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "    user_log_dict = {}\n",
    "    user_count_dict = {} # ä½•æ—¥ç™»å ´ã—ãŸã‹ã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    user_mesh_traj = {}\n",
    "    user_time_traj = {}\n",
    "    user_act_traj = {}\n",
    "    nokitaku_count = 0\n",
    "    all_count = 0\n",
    "    folderlist = sorted(folderlist)\n",
    "\n",
    "    if dataset == 'toyosu_2019/201907-202002' or dataset == '99_202110/logs': \n",
    "        act_dict = act_dict_shibuya21toyosu\n",
    "    elif dataset == '04_202212old' or dataset == '04_202301new' or dataset == '05_202311':\n",
    "        act_dict = act_dict_shibuya2223\n",
    "\n",
    "    for folder in folderlist: # å€‹ã€…ã®æ—¥\n",
    "        if not os.path.isdir(os.path.join(base_path, folder)):\n",
    "            continue\n",
    "\n",
    "        filelist = os.listdir(os.path.join(base_path, folder))\n",
    "        \n",
    "        trip_path = os.path.join(base_path, folder, 't_trip.csv')\n",
    "        loc_path = os.path.join(base_path, folder, 't_loc_data.csv')\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—\n",
    "        if not os.path.exists(trip_path) or not os.path.exists(loc_path):\n",
    "            print(f\"ã‚¹ã‚­ãƒƒãƒ—: {folder}ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰\")\n",
    "            continue\n",
    "\n",
    "        df_trip = pd.read_csv(trip_path, encoding='shift-jis')\n",
    "        df_loc = pd.read_csv(loc_path, encoding='shift-jis')\n",
    "        df_loc['è¨˜éŒ²æ—¥æ™‚'] = pd.to_datetime(df_loc['è¨˜éŒ²æ—¥æ™‚'].str.split('.').str[0], errors='coerce') # ç§’ã®å°æ•°ç‚¹ä»¥ä¸‹ã‚’åˆ‡ã‚Šæ¨ã¦\n",
    "        df_trip['å‡ºç™ºæ™‚åˆ»'] = pd.to_datetime(df_trip['å‡ºç™ºæ™‚åˆ»'])\n",
    "        df_trip['åˆ°ç€æ™‚åˆ»'] = pd.to_datetime(df_trip['åˆ°ç€æ™‚åˆ»'])\n",
    "        grouped = df_trip.groupby('ãƒ¦ãƒ¼ã‚¶ãƒ¼ID')\n",
    "\n",
    "        #### ãƒ¡ãƒƒã‚·ãƒ¥ã‚³ãƒ¼ãƒ‰ã‚’å–å¾— ####        \n",
    "        gdf_points = gpd.GeoDataFrame(df_loc, geometry=gpd.points_from_xy(df_loc['çµŒåº¦'], df_loc['ç·¯åº¦']), crs=\"EPSG:4326\")\n",
    "        gdf_joined = gpd.sjoin(gdf_points, gdf_mesh, predicate='within')\n",
    "        df_loc2 = df_loc.copy()\n",
    "        df_loc2['mesh'] = gdf_joined['3rdmesh']\n",
    "        \n",
    "        for user_id, group in grouped: # å€‹äººã”ã¨\n",
    "            userid = group['ãƒ¦ãƒ¼ã‚¶ãƒ¼ID'].values[0]\n",
    "            group = group.sort_values(by='å‡ºç™ºæ™‚åˆ»')\n",
    "            acts = list(group['ç›®çš„ã‚³ãƒ¼ãƒ‰ï¼ˆactiveï¼‰']) # ç³»åˆ—\n",
    "            times = list(group['å‡ºç™ºæ™‚åˆ»']) # ç³»åˆ—\n",
    "            \n",
    "            #### ç•°å¸¸å€¤æ’é™¤ #### # ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã«ã‚ˆã£ã¦å‡¦ç†ãŒç•°ãªã‚‹\n",
    "            all_count += 1\n",
    "            if dataset == 'toyosu_2019/201907-202002' or dataset == '99_202110/logs': \n",
    "                if acts[-1] != 300 and acts[-1] != 201:\n",
    "                    if acts[-1] == 500 or acts[-1] == 501 or acts[-1] == 999:\n",
    "                        acts[-1] = 300\n",
    "                        # continue\n",
    "                    # print('kitakushitenai!!!')\n",
    "                if acts[-1] != 300 and acts[-1] != 201:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "                # æœ€åˆã‹ã‚‰å¸°å®…ã®äºº\n",
    "                if acts[0] == 300 or acts[0] == 201:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "\n",
    "            elif dataset == '04_202212old' or dataset == '04_202301new' or dataset == '05_202311':\n",
    "                if acts[-1] != 110:\n",
    "                    if acts[-1] == 998 or acts[-1] == 999:\n",
    "                        acts[-1] = 110\n",
    "                if acts[-1] != 110:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "                if acts[0] == 110:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "\n",
    "            locmesh = []\n",
    "            for i, time in enumerate(times):\n",
    "                time = pd.to_datetime(time, errors='coerce')  # â† ã“ã“ã§å¤‰æ›\n",
    "                user_loc_df = df_loc2[df_loc2['ãƒ¦ãƒ¼ã‚¶ãƒ¼ID'] == userid].copy()\n",
    "\n",
    "                if user_loc_df.empty or pd.isna(time):\n",
    "                    continue\n",
    "                # æ™‚åˆ»å·®ã‚’è¨ˆç®—\n",
    "                user_loc_df['time_diff'] = (user_loc_df['è¨˜éŒ²æ—¥æ™‚'] - time).abs()\n",
    "\n",
    "                # æœ€å°å·®åˆ†ã®è¡Œã‚’å–å¾—ï¼ˆNaTã¯è‡ªå‹•é™¤å¤–ã•ã‚Œã‚‹ï¼‰\n",
    "                nearest_row = user_loc_df.loc[user_loc_df['time_diff'].idxmin()]\n",
    "                mesh = nearest_row['mesh']\n",
    "                #print(f\"User ID: {userid}, Time: {time}, mesh: {mesh}\")\n",
    "                locmesh.append(mesh)\n",
    "            \n",
    "            # timeã¯1æ™‚é–“ãŠãã«ã™ã‚‹\n",
    "            times = list(group['å‡ºç™ºæ™‚åˆ»'].dt.floor('h'))\n",
    "            locmesh.append('<e>')\n",
    "            locmesh.insert(0, '<b>') #OK\n",
    "            acts.insert(0, '<b>')\n",
    "            acts.append('<e>')\n",
    "            times.insert(0, '<b>')\n",
    "            times.append('<e>')\n",
    "\n",
    "            if user_id not in list(user_log_dict.keys()):\n",
    "                user_log_dict[user_id] = len(group)\n",
    "                user_count_dict[user_id] = 1\n",
    "                user_mesh_traj[user_id] = locmesh\n",
    "                user_time_traj[user_id] = times\n",
    "                user_act_traj[user_id] = acts\n",
    "\n",
    "            else:\n",
    "                user_log_dict[user_id] += len(group)\n",
    "                user_count_dict[user_id] += 1\n",
    "                user_mesh_traj[user_id] += locmesh\n",
    "                user_time_traj[user_id] += times\n",
    "                user_act_traj[user_id] += acts\n",
    "\n",
    "    # insertã—ãŸusrer_idãŒindexã«ãªã‚‰ãªã„ã‚ˆã†ã«å‡¦ç†\n",
    "    # df_mesh_traj = pd.DataFrame.from_dict(user_mesh_traj, orient='index')\n",
    "    df_mesh_traj = pd.DataFrame.from_dict(user_mesh_traj, orient='index').reset_index(names='user_id')\n",
    "    # df_time_traj = pd.DataFrame.from_dict(user_time_traj, orient='index')\n",
    "    df_time_traj = pd.DataFrame.from_dict(user_time_traj, orient='index').reset_index(names='user_id')\n",
    "    # df_act_traj = pd.DataFrame.from_dict(user_act_traj, orient='index')\n",
    "    df_act_traj = pd.DataFrame.from_dict(user_act_traj, orient='index').reset_index(names='user_id')\n",
    "\n",
    "    # print('1', df_mesh_traj.head(5))\n",
    "    # print('2test', df_mesh_traj.iloc[0].tolist())\n",
    "\n",
    "    # df_mesh_traj.insert(0, 'user_id', df_mesh_traj.index)\n",
    "    # df_time_traj.insert(0, 'user_id', df_time_traj.index)\n",
    "    # df_act_traj.insert(0, 'user_id', df_act_traj.index)\n",
    "\n",
    "    #### æ´»å‹•ãƒ©ãƒ™ãƒ«ã‚’é›†ç´„ ####\n",
    "    df_act_traj = df_act_traj.applymap(try_convert_int)\n",
    "    df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
    "\n",
    "    # print('1', df_mesh_traj.head(5))\n",
    "    # print('2test', df_mesh_traj.iloc[0].tolist())\n",
    "    # print()\n",
    "    \n",
    "    #### æ´»å‹•å ´æ‰€ã¨ç¨®é¡ãŒé€£ç¶šã—ã¦ã„ã‚‹å ´åˆã¯å‰Šé™¤ ####\n",
    "    # æœ€å¤§åˆ—æ•°ï¼ˆä¸è¶³åˆ†ã‚’åŸ‹ã‚ã‚‹ç”¨ï¼‰\n",
    "    max_len = max(df_act_traj.shape[1], df_time_traj.shape[1], df_mesh_traj.shape[1])\n",
    "\n",
    "    # NaNã‚’åŸ‹ã‚ã‚‹ï¼ˆåˆ—æ•°ã‚’æƒãˆã‚‹ï¼‰\n",
    "    # df_act_traj = df_act_traj.reindex(columns=range(max_len))\n",
    "    # df_time_traj = df_time_traj.reindex(columns=range(max_len))\n",
    "    # df_mesh_traj = df_mesh_traj.reindex(columns=range(max_len))\n",
    "    \n",
    "    # 'user_id' ä»¥å¤–ã®åˆ—ã ã‘ã‚’å¯¾è±¡ã« reindex ã—ã¦ã‹ã‚‰çµåˆ\n",
    "    id_col = df_act_traj[['user_id']]\n",
    "    act_cols = df_act_traj.drop(columns='user_id')\n",
    "    act_cols = act_cols.reindex(columns=range(max_len))\n",
    "    df_act_traj = pd.concat([id_col, act_cols], axis=1)\n",
    "\n",
    "    # åŒæ§˜ã«\n",
    "    id_col = df_time_traj[['user_id']]\n",
    "    time_cols = df_time_traj.drop(columns='user_id')\n",
    "    time_cols = time_cols.reindex(columns=range(max_len))\n",
    "    df_time_traj = pd.concat([id_col, time_cols], axis=1)\n",
    "\n",
    "    id_col = df_mesh_traj[['user_id']]\n",
    "    mesh_cols = df_mesh_traj.drop(columns='user_id')\n",
    "    mesh_cols = mesh_cols.reindex(columns=range(max_len))\n",
    "    df_mesh_traj = pd.concat([id_col, mesh_cols], axis=1)\n",
    "    \n",
    "    # print('2', df_mesh_traj.head(5))\n",
    "    act_cleaned = []\n",
    "    time_cleaned = []\n",
    "    mesh_cleaned = []\n",
    "\n",
    "    # print('2222df_act_traj', df_act_traj.iloc[0].tolist())\n",
    "    # print('222', df_time_traj.columns)\n",
    "    # sys.exit()\n",
    "\n",
    "    for i in range(len(df_act_traj)):\n",
    "        a_row = df_act_traj.iloc[i].tolist()\n",
    "        t_row = df_time_traj.iloc[i].tolist()\n",
    "        m_row = df_mesh_traj.iloc[i].tolist()\n",
    "        a_new, t_new, m_new = remove_consecutive_duplicates(a_row, t_row, m_row)\n",
    "        act_cleaned.append(a_new)\n",
    "        time_cleaned.append(t_new)\n",
    "        mesh_cleaned.append(m_new)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–ã—ã¦ä¿å­˜\n",
    "    df_act_cleaned = pd.DataFrame(act_cleaned)\n",
    "    df_time_cleaned = pd.DataFrame(time_cleaned)\n",
    "    df_mesh_cleaned = pd.DataFrame(mesh_cleaned)\n",
    "    # print('3', df_mesh_cleaned.head(5))\n",
    "    # sys.exit()\n",
    "\n",
    "    df_mesh_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'meshtraj{nameid_list[datasetid]}.csv'), index=False) \n",
    "    df_time_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'timetraj{nameid_list[datasetid]}.csv'), index=False)\n",
    "    df_act_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'acttraj{nameid_list[datasetid]}.csv'), index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce205a",
   "metadata": {},
   "source": [
    "1é€±é–“ã”ã¨ã«åˆ†å‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¤é–¢æ•°ã®ç”¨æ„\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "\n",
    "def split_line_to_segments(line):\n",
    "    \"\"\"\n",
    "    å„è¡Œã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ '<b>'ï¼'<e>' ã§åˆ†å‰²ã—ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’è¿”ã™\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    current = []\n",
    "    for token in line:\n",
    "        if token == '<b>':\n",
    "            current = ['<b>']\n",
    "        elif token == '<e>':\n",
    "            if current:\n",
    "                current.append('<e>')\n",
    "                segments.append(current)\n",
    "        else:\n",
    "            current.append(token)\n",
    "    return segments \n",
    "\n",
    "\n",
    "def flatten_chunks(chunks):\n",
    "    \"\"\"\n",
    "    ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ '<b>' ã¨ '<e>' ä»˜ãã®ãƒ•ãƒ©ãƒƒãƒˆãªè¡Œãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for chunk in chunks:\n",
    "        row = ['<b>']\n",
    "        for seg in chunk:\n",
    "            row.extend(seg)\n",
    "        row.append('<e>')\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def date_of_time_seg(seg):\n",
    "    \"\"\"æ™‚é–“ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º\"\"\"\n",
    "    return datetime.strptime(seg[1], \"%Y-%m-%d %H:%M:%S\").date() # æœ€åˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã ã‘idãŒå…¥ã£ã¦ã‚‹ã®ã§3ç•ªç›®ã®é …ã‹ã‚‰å§‹ã¾ã‚‹\n",
    "\n",
    "\n",
    "def group_into_weeks(segments): #, date_extractor):\n",
    "    \"\"\"\n",
    "    7æ—¥é–“ã”ã¨ã«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "    \"\"\"\n",
    "    if not segments:\n",
    "        return []\n",
    "    chunks = []\n",
    "    current = []\n",
    "    start_date = datetime.strptime(segments[0][2], \"%Y-%m-%d %H:%M:%S\").date() # æœ€åˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã ã‘idãŒå…¥ã£ã¦ã‚‹ã®ã§3ç•ªç›®ã®é …ã‹ã‚‰å§‹ã¾ã‚‹ ok\n",
    "\n",
    "    for i, seg in enumerate(segments):\n",
    "        d = date_of_time_seg(seg)\n",
    "        if d < start_date + timedelta(days=7):\n",
    "            current.append(seg)\n",
    "        else:\n",
    "            chunks.append(current)\n",
    "            current = [seg]\n",
    "            start_date = d\n",
    "        \n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def fill_missing_days(weekly_time_segs, weekly_act_segs, weekly_mesh_segs):\n",
    "    \"\"\"\n",
    "    æ¬ è½æ—¥ã«ã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’æŒ¿å…¥ï¼ˆtime: æ—¥ä»˜ãƒˆãƒ¼ã‚¯ãƒ³ / act: '955'ï¼‰\n",
    "    \"\"\"\n",
    "    # date -> (time_seg, act_seg)\n",
    "    date_map = {}\n",
    "    for t_seg, a_seg, m_seg in zip(weekly_time_segs, weekly_act_segs, weekly_mesh_segs):\n",
    "        date_map[date_of_time_seg(t_seg)] = (t_seg, a_seg, m_seg)\n",
    "\n",
    "    start = date_of_time_seg(weekly_time_segs[0])\n",
    "    filled_time, filled_act, filled_mesh = [], [], []\n",
    "    for i in range(7):\n",
    "        d = start + timedelta(days=i)\n",
    "        if d in date_map:\n",
    "            t_seg, a_seg, m_seg = date_map[d]\n",
    "        else:\n",
    "            t_seg = ['<b>', d.strftime(\"%Y-%m-%d\"), '<e>']\n",
    "            a_seg = ['<b>', \"955\", '<e>']\n",
    "            m_seg = ['<b>', \"999999\", '<e>']\n",
    "\n",
    "        filled_time.append(t_seg)\n",
    "        filled_act.append(a_seg)\n",
    "        filled_mesh.append(m_seg)\n",
    "\n",
    "    return filled_time, filled_act, filled_mesh\n",
    "\n",
    "\n",
    "def write_chunks_to_csv(chunks, output_path):\n",
    "    with open(output_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for segs in chunks:\n",
    "            row = ['<b>'] + [token for seg in segs for token in seg] + ['<e>']\n",
    "            writer.writerow(row[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦å®Ÿè¡Œ\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "    act_file = (os.path.join(input_path, f'acttraj{nameid_list[datasetid]}.csv'))\n",
    "    time_file = (os.path.join(input_path, f'timetraj{nameid_list[datasetid]}.csv'))\n",
    "    mesh_file = (os.path.join(input_path, f'meshtraj{nameid_list[datasetid]}.csv'))\n",
    "\n",
    "    out_time_path = os.path.join(input_path, f'time_weekly{nameid_list[datasetid]}.csv')\n",
    "    out_act_path = os.path.join(input_path, f'act_weekly{nameid_list[datasetid]}.csv')\n",
    "    out_mesh_path = os.path.join(input_path, f'mesh_weekly{nameid_list[datasetid]}.csv')\n",
    "    \n",
    "    # csvã‚’èª­ã¿è¾¼ã‚€ï¼šå…ˆé ­ã¯useridãªã®ã§é£›ã°ã™\n",
    "    with open(time_file, newline='') as f:\n",
    "        time_rows = list(csv.reader(f))[1:]\n",
    "    with open(act_file, newline='') as f:\n",
    "        act_rows = list(csv.reader(f))[1:]\n",
    "    with open(mesh_file, newline='') as f:\n",
    "        mesh_rows = list(csv.reader(f))[1:]\n",
    "\n",
    "    all_time_chunks, all_act_chunks, all_mesh_chunks = [], [], []\n",
    "    count = 0 \n",
    "    for t_row, a_row, m_row in zip(time_rows, act_rows, mesh_rows): # å€‹äººã”ã¨\n",
    "        t_segs = split_line_to_segments(t_row) # idã¯æ¶ˆãˆã¦ã‚‹\n",
    "        a_segs = split_line_to_segments(a_row)\n",
    "        m_segs = split_line_to_segments(m_row)\n",
    "\n",
    "        ## ã“ã‚Œã‚ã‚‹ï¼Ÿï¼Ÿ\n",
    "        userid = (t_row[0]) # fixed\n",
    "        weekly_ts = group_into_weeks(t_segs) # æ—¥æ¯ã®b-eã®ãƒªã‚¹ãƒˆ\n",
    "        idx_map = {tuple(seg): idx for idx, seg in enumerate(t_segs)}\n",
    "\n",
    "        for week in weekly_ts: # 1é€±é–“ã”ã¨ã«ãªã£ã¦ã„ã‚‹\n",
    "            # å…ƒãƒ‡ãƒ¼ã‚¿ã®actã‚’å¯¾å¿œã¥ã‘\n",
    "            act_week = []\n",
    "            mesh_week = []\n",
    "            for seg in week:\n",
    "                idx = idx_map.get(tuple(seg))\n",
    "                act_week.append(a_segs[idx] if idx is not None else [\"955\"])\n",
    "                mesh_week.append(m_segs[idx] if idx is not None else [\"999999\"])\n",
    "\n",
    "            # æ¬ è½æ—¥ã®è£œå®Œ\n",
    "            filled_t, filled_a, filled_m = fill_missing_days(week, act_week, mesh_week)\n",
    "\n",
    "            filled_t[0].insert(0, f'{userid}')\n",
    "            filled_a[0].insert(0, f'{userid}')\n",
    "            filled_m[0].insert(0, f'{userid}')\n",
    "\n",
    "            all_time_chunks.append(filled_t)\n",
    "            all_act_chunks.append(filled_a)\n",
    "            all_mesh_chunks.append(filled_m)\n",
    "\n",
    "    write_chunks_to_csv(all_time_chunks, out_time_path)\n",
    "    write_chunks_to_csv(all_act_chunks, out_act_path)\n",
    "    write_chunks_to_csv(all_mesh_chunks, out_mesh_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe9e08",
   "metadata": {},
   "source": [
    "é•·ã•ã‚’æƒãˆã¦ã‹ã‚‰ï¼Œ5ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§± ç·è¡Œæ•°: 3992, 3992, 3992\n"
     ]
    }
   ],
   "source": [
    "act_all_rows = [] \n",
    "time_all_rows = [] \n",
    "mesh_all_rows = [] \n",
    "\n",
    "act_max_cols = 0\n",
    "time_max_cols = 0\n",
    "mesh_max_cols = 0\n",
    "\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "\n",
    "    time_path = os.path.join(input_path, f'time_weekly{nameid_list[datasetid]}.csv')\n",
    "    act_path = os.path.join(input_path, f'act_weekly{nameid_list[datasetid]}.csv')\n",
    "    mesh_path = os.path.join(input_path, f'mesh_weekly{nameid_list[datasetid]}.csv')\n",
    "\n",
    "    # èª­ã¿è¾¼ã¿ + ç©ºæ–‡å­—ãƒ»Noneãƒ»\"NaN\"é™¤å»\n",
    "    with open(act_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        act_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "    \n",
    "    with open(time_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        time_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "\n",
    "    with open(mesh_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        mesh_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "    \n",
    "    # æœ€å¤§åˆ—æ•°ã‚’æ›´æ–°\n",
    "    for row in act_cleaned_rows:\n",
    "        act_max_cols = max(act_max_cols, len(row))\n",
    "    \n",
    "    for row in time_cleaned_rows:\n",
    "        time_max_cols = max(time_max_cols, len(row))\n",
    "\n",
    "    for row in mesh_cleaned_rows:\n",
    "        mesh_max_cols = max(mesh_max_cols, len(row))\n",
    "    \n",
    "    act_all_rows.extend(act_cleaned_rows)\n",
    "    time_all_rows.extend(time_cleaned_rows)\n",
    "    mesh_all_rows.extend(mesh_cleaned_rows)\n",
    "\n",
    "# è¡Œã®é•·ã•ã‚’æœ€å¤§åˆ—æ•°ã«ãã‚ãˆã€<p>ã§åŸ‹ã‚ã‚‹\n",
    "act_padded_rows = [row + ['<p>'] * (act_max_cols - len(row)) for row in act_all_rows]\n",
    "time_padded_rows = [row + ['<p>'] * (time_max_cols - len(row)) for row in time_all_rows]\n",
    "mesh_padded_rows = [row + ['<p>'] * (mesh_max_cols - len(row)) for row in mesh_all_rows]\n",
    "\n",
    "# çµåˆçµæœã‚’ä¿å­˜\n",
    "act_output_path = os.path.join(input_path, 'act_weekly_filled.csv')\n",
    "time_output_path = os.path.join(input_path, 'time_weekly_filled.csv')\n",
    "mesh_output_path = os.path.join(input_path, 'mesh_weekly_filled.csv')\n",
    "\n",
    "with open(act_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(act_padded_rows)\n",
    "\n",
    "with open(time_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(time_padded_rows)\n",
    "\n",
    "with open(mesh_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(mesh_padded_rows)\n",
    "\n",
    "print(f\"ğŸ§± ç·è¡Œæ•°: {len(act_padded_rows)}, {len(time_padded_rows)}, {len(mesh_padded_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f1e77a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1425381383.py:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_time = df_time.applymap(lambda x: convert_datetime_to_hour(x) if pd.notna(x) else x)\n"
     ]
    }
   ],
   "source": [
    "def convert_datetime_to_hour(token):\n",
    "    try:\n",
    "        # æ™‚åˆ»ãŒå«ã¾ã‚Œã‚‹å ´åˆï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä»˜ãæ—¥ä»˜ï¼‰\n",
    "        dt = pd.to_datetime(token, errors='raise')\n",
    "        if dt.hour == 0 and \"00:00:00\" not in token:\n",
    "            return 25 # token  # ãŸã¨ãˆã° \"2022-12-21\" ã®ã‚ˆã†ãªå ´åˆ\n",
    "        return str(dt.hour)\n",
    "    except Exception:\n",
    "        # å¤‰æ›ã§ããªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™\n",
    "        return token\n",
    "## \n",
    "df_time = pd.read_csv(os.path.join(input_path, 'time_weekly_filled.csv'), header=None)\n",
    "df_time = df_time.applymap(lambda x: convert_datetime_to_hour(x) if pd.notna(x) else x)\n",
    "df_time.to_csv(os.path.join(input_path, 'time_weekly_filled_hour.csv'), index=False, header=False)\n",
    "# converted_row = [convert_datetime_to_hour(tok) for tok in row]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fcf11",
   "metadata": {},
   "source": [
    "æ—¥ä»˜å¤‰æ›´æ™‚ç‚¹ã‚’Sãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0f8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act = pd.read_csv(os.path.join(input_path, 'act_weekly_filled.csv'), header=None)\n",
    "df_time = pd.read_csv(os.path.join(input_path, 'time_weekly_filled_hour.csv'), header=None)\n",
    "df_mesh = pd.read_csv(os.path.join(input_path, 'mesh_weekly_filled.csv'), header=None)\n",
    "\n",
    "# å„è¡Œè¦‹ã¦ã„ã£ã¦ï¼Œ<b>ã‚’æ¶ˆã™ï¼†<e>ã‚’<S>ã«ç½®æ›\n",
    "df_act = df_act.replace('<e>', '<s>')\n",
    "df_time = df_time.replace('<e>', '<s>')\n",
    "df_mesh = df_mesh.replace('<e>', '<s>')\n",
    "\n",
    "# ç‰¹å®šã®æ–‡å­—ã‚’å‰Šé™¤ã—ã¦å·¦è©°ã‚ï¼ˆæ–‡å­—åˆ—çµåˆã§å†æ§‹æˆï¼‰\n",
    "def remove_and_shift(row, char='<b>'):\n",
    "    # å„ã‚»ãƒ«ã‹ã‚‰ç‰¹å®šæ–‡å­—ã‚’å‰Šé™¤ã—ã€è¡Œã‚’å†æ§‹æˆ\n",
    "    cleaned = [str(cell).replace(char, '') for cell in row]\n",
    "    # ç©ºã§ãªã„ã‚‚ã®ã‚’å·¦ã«å¯„ã›ã€ç©ºæ–‡å­—ã‚’å³ã«è©°ã‚ã‚‹\n",
    "    non_empty = [c for c in cleaned if c]\n",
    "    empty = [''] * (len(row) - len(non_empty))\n",
    "    return pd.Series(non_empty + empty)\n",
    "\n",
    "# å„è¡Œã«é©ç”¨\n",
    "df_act_cleaned = df_act.apply(remove_and_shift, axis=1)\n",
    "df_time_cleaned = df_time.apply(remove_and_shift, axis=1)\n",
    "df_mesh_cleaned = df_mesh.apply(remove_and_shift, axis=1)\n",
    "\n",
    "df_act_cleaned = df_act_cleaned.fillna('<p>')\n",
    "df_time_cleaned = df_time_cleaned.fillna('<p>')\n",
    "df_mesh_cleaned = df_mesh_cleaned.fillna('<p>')\n",
    "# print(df_act_cleaned)\n",
    "\n",
    "df_act_cleaned = df_act_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "df_time_cleaned = df_time_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "df_mesh_cleaned = df_mesh_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "\n",
    "df_act = df_act_cleaned\n",
    "df_time = df_time_cleaned\n",
    "df_mesh = df_mesh_cleaned\n",
    "\n",
    "#### ã“ã“ã‹ã‚‰ <s> ã‚’ <s1>, <s2>, ... ã«å¤‰æ› ####\n",
    "def convert_d(row):\n",
    "    # row ã”ã¨ã«ã‚«ã‚¦ãƒ³ãƒˆå¤‰ãˆã‚‹ã®ã§\n",
    "    count = 0\n",
    "    for i in range(len(row)):\n",
    "        if row[i] == '<s>':\n",
    "            count += 1\n",
    "            if count <= 6: # countãŒ7, 8ã®æ™‚ã¯ä¸€ç•ªå¾Œã‚ã®ã‚„ã¤ãªã®ã§ç„¡è¦–ã™ã‚‹\n",
    "                row[i] = '<s' + str(count) + '>'\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return row\n",
    "\n",
    "for i in range(len(df_time)):\n",
    "    # 1è¡Œãšã¤å–ã‚Šå‡ºã™\n",
    "    row = df_time.iloc[i]\n",
    "    # 1è¡Œã®ä¸­ã«<s>ãŒã„ãã¤ã‚ã‚‹ã‹ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    row = convert_d(row)\n",
    "    # å¤‰æ›ã—ãŸè¡Œã‚’å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æˆ»ã™\n",
    "    df_time.iloc[i] = row\n",
    "\n",
    "for i in range(len(df_act)):\n",
    "    # 1è¡Œãšã¤å–ã‚Šå‡ºã™\n",
    "    row = df_act.iloc[i]\n",
    "    # 1è¡Œã®ä¸­ã«<s>ãŒã„ãã¤ã‚ã‚‹ã‹ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    row = convert_d(row)\n",
    "    # å¤‰æ›ã—ãŸè¡Œã‚’å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æˆ»ã™\n",
    "    df_act.iloc[i] = row\n",
    "\n",
    "for i in range(len(df_mesh)):\n",
    "    # 1è¡Œãšã¤å–ã‚Šå‡ºã™\n",
    "    row = df_mesh.iloc[i]\n",
    "    # 1è¡Œã®ä¸­ã«<s>ãŒã„ãã¤ã‚ã‚‹ã‹ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    row = convert_d(row)\n",
    "    # å¤‰æ›ã—ãŸè¡Œã‚’å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æˆ»ã™\n",
    "    df_mesh.iloc[i] = row\n",
    "\n",
    "df_time.to_csv(os.path.join(input_path, 'time_weekly_filled_structured.csv'), index=False)\n",
    "df_act.to_csv(os.path.join(input_path, 'act_weekly_filled_structured.csv'), index=False)\n",
    "df_mesh.to_csv(os.path.join(input_path, 'mesh_weekly_filled_structured.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbef81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
