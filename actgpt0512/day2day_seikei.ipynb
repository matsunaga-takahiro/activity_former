{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da038142",
   "metadata": {},
   "source": [
    " PPからlocationシーケンスの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e521eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import sys\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b72ad8",
   "metadata": {},
   "source": [
    "time, mesh, actの個人ごとのトータルシーケンスを作成（この後に1週間ごとにデータ分割）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c20a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip dataの出発時刻に併せて loc_dataからlon, latを取り出す\n",
    "base_path0 = '/Users/matsunagatakahiro/Desktop/jrres/PPcameraTG/gpslog'#/04_202212old'\n",
    "input_path = '/Users/matsunagatakahiro/Desktop/res2025/ActFormer/RoutesFormer/actgpt0512/input'\n",
    "dataset_list = ['04_202212old', '04_202301new', '05_202311', 'toyosu_2019/201907-202002', '99_202110/logs']\n",
    "nameid_list = ['01', '02', '03', 'toyosu', 'shibu21']\n",
    "gdf_mesh1 = gpd.read_file(os.path.join(base_path0, '3rdmesh5339/MESH05339.shp'))\n",
    "gdf_mesh2 = gpd.read_file(os.path.join(base_path0, '3rdmesh5340/MESH05340.shp'))\n",
    "gdf_mesh3 = gpd.read_file(os.path.join(base_path0, '3rdmesh5440/MESH05440.shp'))\n",
    "gdf_mesh4 = gpd.read_file(os.path.join(base_path0, '3rdmesh5239/MESH05239.shp'))\n",
    "gdf_mesh5 = gpd.read_file(os.path.join(base_path0, '3rdmesh5439/MESH05439.shp'))\n",
    "\n",
    "gdf_mesh = pd.concat([gdf_mesh1, gdf_mesh2, gdf_mesh3, gdf_mesh4, gdf_mesh5], ignore_index=True)\n",
    "gdf_mesh = gdf_mesh.to_crs(\"EPSG:4326\") # WGS84に変換\n",
    "gdf_mesh['3rdmesh'] = gdf_mesh['KEY_CODE'].astype(str).str[0:6] # 3次メッシュコード\n",
    "\n",
    "act_dict_shibuya2223 = {\n",
    "    110: 1, # H\n",
    "    200: 2, # S\n",
    "    210: 2, # S\n",
    "    220: 1, # hotel->home\n",
    "    300: 3, # W \n",
    "    310: 4, # W2\n",
    "    400: 3, # W\n",
    "    998: 5, # Other\n",
    "    999: 5 # , # Other\n",
    "}\n",
    "\n",
    "act_dict_shibuya21toyosu = {\n",
    "    300: 1, # H\n",
    "    201: 1, # H\n",
    "    400: 2, # S\n",
    "    401: 2, \n",
    "    402: 2, \n",
    "    404: 2, \n",
    "    405: 2, \n",
    "    406: 2, \n",
    "    100: 3, # W\n",
    "    200: 4, # W2\n",
    "    403: 5, # Ohter\n",
    "    407: 5, # Ohter\n",
    "    500: 5, # Ohter\n",
    "    501: 5, # Ohter\n",
    "    999: 5 #, # Ohter`\n",
    "}\n",
    "\n",
    "def remove_consecutive_duplicates(act_row, time_row, mesh_row):\n",
    "    # print('remove_consecutive_duplicates', act_row, time_row, mesh_row)\n",
    "    new_act = [act_row[0]]\n",
    "    new_time = [time_row[0]]\n",
    "    new_mesh = [mesh_row[0]]\n",
    "    for i in range(1, len(act_row)):\n",
    "        if not (act_row[i] == act_row[i-1] and mesh_row[i] == mesh_row[i-1]):\n",
    "            new_act.append(act_row[i])\n",
    "            new_time.append(time_row[i])\n",
    "            new_mesh.append(mesh_row[i])\n",
    "    return pd.Series(new_act), pd.Series(new_time), pd.Series(new_mesh)\n",
    "\n",
    "def try_convert_int(x):\n",
    "    if x in ['<b>', '<e>']:\n",
    "        return x\n",
    "    try:\n",
    "        return int(float(x))  # 300.0 → 300 などにも対応\n",
    "    except:\n",
    "        return x  # 変換できなければそのまま\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfe4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スキップ: 10004_20190915（ファイルが存在しない）\n",
      "スキップ: 10004_20190916（ファイルが存在しない）\n",
      "スキップ: 10004_20191020（ファイルが存在しない）\n",
      "スキップ: 10004_20191229（ファイルが存在しない）\n",
      "スキップ: 10004_20191230（ファイルが存在しない）\n",
      "スキップ: 10004_20191231（ファイルが存在しない）\n",
      "スキップ: 10004_20200101（ファイルが存在しない）\n",
      "スキップ: 10004_20200102（ファイルが存在しない）\n",
      "スキップ: 10004_20200103（ファイルが存在しない）\n",
      "スキップ: 10004_20200104（ファイルが存在しない）\n",
      "スキップ: 10004_20200105（ファイルが存在しない）\n",
      "スキップ: 10004_20200111（ファイルが存在しない）\n",
      "スキップ: 10004_20200112（ファイルが存在しない）\n",
      "スキップ: 10004_20200113（ファイルが存在しない）\n",
      "スキップ: 10004_20200126（ファイルが存在しない）\n",
      "スキップ: 10004_20200127（ファイルが存在しない）\n",
      "スキップ: 10004_20200201（ファイルが存在しない）\n",
      "スキップ: 10004_20200202（ファイルが存在しない）\n",
      "スキップ: 10004_20200208（ファイルが存在しない）\n",
      "スキップ: 10004_20200209（ファイルが存在しない）\n",
      "スキップ: 10004_20200210（ファイルが存在しない）\n",
      "スキップ: 10004_20200211（ファイルが存在しない）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:197: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(try_convert_int)\n",
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1373987240.py:198: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "    user_log_dict = {}\n",
    "    user_count_dict = {} # 何日登場したかのカウント\n",
    "    user_mesh_traj = {}\n",
    "    user_time_traj = {}\n",
    "    user_act_traj = {}\n",
    "    nokitaku_count = 0\n",
    "    all_count = 0\n",
    "    folderlist = sorted(folderlist)\n",
    "\n",
    "    if dataset == 'toyosu_2019/201907-202002' or dataset == '99_202110/logs': \n",
    "        act_dict = act_dict_shibuya21toyosu\n",
    "    elif dataset == '04_202212old' or dataset == '04_202301new' or dataset == '05_202311':\n",
    "        act_dict = act_dict_shibuya2223\n",
    "\n",
    "    for folder in folderlist: # 個々の日\n",
    "        if not os.path.isdir(os.path.join(base_path, folder)):\n",
    "            continue\n",
    "\n",
    "        filelist = os.listdir(os.path.join(base_path, folder))\n",
    "        \n",
    "        trip_path = os.path.join(base_path, folder, 't_trip.csv')\n",
    "        loc_path = os.path.join(base_path, folder, 't_loc_data.csv')\n",
    "        # ファイルがなければスキップ\n",
    "        if not os.path.exists(trip_path) or not os.path.exists(loc_path):\n",
    "            print(f\"スキップ: {folder}（ファイルが存在しない）\")\n",
    "            continue\n",
    "\n",
    "        df_trip = pd.read_csv(trip_path, encoding='shift-jis')\n",
    "        df_loc = pd.read_csv(loc_path, encoding='shift-jis')\n",
    "        df_loc['記録日時'] = pd.to_datetime(df_loc['記録日時'].str.split('.').str[0], errors='coerce') # 秒の小数点以下を切り捨て\n",
    "        df_trip['出発時刻'] = pd.to_datetime(df_trip['出発時刻'])\n",
    "        df_trip['到着時刻'] = pd.to_datetime(df_trip['到着時刻'])\n",
    "        grouped = df_trip.groupby('ユーザーID')\n",
    "\n",
    "        #### メッシュコードを取得 ####        \n",
    "        gdf_points = gpd.GeoDataFrame(df_loc, geometry=gpd.points_from_xy(df_loc['経度'], df_loc['緯度']), crs=\"EPSG:4326\")\n",
    "        gdf_joined = gpd.sjoin(gdf_points, gdf_mesh, predicate='within')\n",
    "        df_loc2 = df_loc.copy()\n",
    "        df_loc2['mesh'] = gdf_joined['3rdmesh']\n",
    "        \n",
    "        for user_id, group in grouped: # 個人ごと\n",
    "            userid = group['ユーザーID'].values[0]\n",
    "            group = group.sort_values(by='出発時刻')\n",
    "            acts = list(group['目的コード（active）']) # 系列\n",
    "            times = list(group['出発時刻']) # 系列\n",
    "            \n",
    "            #### 異常値排除 #### # タイミングによって処理が異なる\n",
    "            all_count += 1\n",
    "            if dataset == 'toyosu_2019/201907-202002' or dataset == '99_202110/logs': \n",
    "                if acts[-1] != 300 and acts[-1] != 201:\n",
    "                    if acts[-1] == 500 or acts[-1] == 501 or acts[-1] == 999:\n",
    "                        acts[-1] = 300\n",
    "                        # continue\n",
    "                    # print('kitakushitenai!!!')\n",
    "                if acts[-1] != 300 and acts[-1] != 201:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "                # 最初から帰宅の人\n",
    "                if acts[0] == 300 or acts[0] == 201:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "\n",
    "            elif dataset == '04_202212old' or dataset == '04_202301new' or dataset == '05_202311':\n",
    "                if acts[-1] != 110:\n",
    "                    if acts[-1] == 998 or acts[-1] == 999:\n",
    "                        acts[-1] = 110\n",
    "                if acts[-1] != 110:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "                if acts[0] == 110:\n",
    "                    nokitaku_count += 1\n",
    "                    continue\n",
    "\n",
    "            locmesh = []\n",
    "            for i, time in enumerate(times):\n",
    "                time = pd.to_datetime(time, errors='coerce')  # ← ここで変換\n",
    "                user_loc_df = df_loc2[df_loc2['ユーザーID'] == userid].copy()\n",
    "\n",
    "                if user_loc_df.empty or pd.isna(time):\n",
    "                    continue\n",
    "                # 時刻差を計算\n",
    "                user_loc_df['time_diff'] = (user_loc_df['記録日時'] - time).abs()\n",
    "\n",
    "                # 最小差分の行を取得（NaTは自動除外される）\n",
    "                nearest_row = user_loc_df.loc[user_loc_df['time_diff'].idxmin()]\n",
    "                mesh = nearest_row['mesh']\n",
    "                #print(f\"User ID: {userid}, Time: {time}, mesh: {mesh}\")\n",
    "                locmesh.append(mesh)\n",
    "            \n",
    "            # timeは1時間おきにする\n",
    "            times = list(group['出発時刻'].dt.floor('h'))\n",
    "            locmesh.append('<e>')\n",
    "            locmesh.insert(0, '<b>') #OK\n",
    "            acts.insert(0, '<b>')\n",
    "            acts.append('<e>')\n",
    "            times.insert(0, '<b>')\n",
    "            times.append('<e>')\n",
    "\n",
    "            if user_id not in list(user_log_dict.keys()):\n",
    "                user_log_dict[user_id] = len(group)\n",
    "                user_count_dict[user_id] = 1\n",
    "                user_mesh_traj[user_id] = locmesh\n",
    "                user_time_traj[user_id] = times\n",
    "                user_act_traj[user_id] = acts\n",
    "\n",
    "            else:\n",
    "                user_log_dict[user_id] += len(group)\n",
    "                user_count_dict[user_id] += 1\n",
    "                user_mesh_traj[user_id] += locmesh\n",
    "                user_time_traj[user_id] += times\n",
    "                user_act_traj[user_id] += acts\n",
    "\n",
    "    # insertしたusrer_idがindexにならないように処理\n",
    "    # df_mesh_traj = pd.DataFrame.from_dict(user_mesh_traj, orient='index')\n",
    "    df_mesh_traj = pd.DataFrame.from_dict(user_mesh_traj, orient='index').reset_index(names='user_id')\n",
    "    # df_time_traj = pd.DataFrame.from_dict(user_time_traj, orient='index')\n",
    "    df_time_traj = pd.DataFrame.from_dict(user_time_traj, orient='index').reset_index(names='user_id')\n",
    "    # df_act_traj = pd.DataFrame.from_dict(user_act_traj, orient='index')\n",
    "    df_act_traj = pd.DataFrame.from_dict(user_act_traj, orient='index').reset_index(names='user_id')\n",
    "\n",
    "    # print('1', df_mesh_traj.head(5))\n",
    "    # print('2test', df_mesh_traj.iloc[0].tolist())\n",
    "\n",
    "    # df_mesh_traj.insert(0, 'user_id', df_mesh_traj.index)\n",
    "    # df_time_traj.insert(0, 'user_id', df_time_traj.index)\n",
    "    # df_act_traj.insert(0, 'user_id', df_act_traj.index)\n",
    "\n",
    "    #### 活動ラベルを集約 ####\n",
    "    df_act_traj = df_act_traj.applymap(try_convert_int)\n",
    "    df_act_traj = df_act_traj.applymap(lambda x: act_dict.get(x, x))\n",
    "\n",
    "    # print('1', df_mesh_traj.head(5))\n",
    "    # print('2test', df_mesh_traj.iloc[0].tolist())\n",
    "    # print()\n",
    "    \n",
    "    #### 活動場所と種類が連続している場合は削除 ####\n",
    "    # 最大列数（不足分を埋める用）\n",
    "    max_len = max(df_act_traj.shape[1], df_time_traj.shape[1], df_mesh_traj.shape[1])\n",
    "\n",
    "    # NaNを埋める（列数を揃える）\n",
    "    # df_act_traj = df_act_traj.reindex(columns=range(max_len))\n",
    "    # df_time_traj = df_time_traj.reindex(columns=range(max_len))\n",
    "    # df_mesh_traj = df_mesh_traj.reindex(columns=range(max_len))\n",
    "    \n",
    "    # 'user_id' 以外の列だけを対象に reindex してから結合\n",
    "    id_col = df_act_traj[['user_id']]\n",
    "    act_cols = df_act_traj.drop(columns='user_id')\n",
    "    act_cols = act_cols.reindex(columns=range(max_len))\n",
    "    df_act_traj = pd.concat([id_col, act_cols], axis=1)\n",
    "\n",
    "    # 同様に\n",
    "    id_col = df_time_traj[['user_id']]\n",
    "    time_cols = df_time_traj.drop(columns='user_id')\n",
    "    time_cols = time_cols.reindex(columns=range(max_len))\n",
    "    df_time_traj = pd.concat([id_col, time_cols], axis=1)\n",
    "\n",
    "    id_col = df_mesh_traj[['user_id']]\n",
    "    mesh_cols = df_mesh_traj.drop(columns='user_id')\n",
    "    mesh_cols = mesh_cols.reindex(columns=range(max_len))\n",
    "    df_mesh_traj = pd.concat([id_col, mesh_cols], axis=1)\n",
    "    \n",
    "    # print('2', df_mesh_traj.head(5))\n",
    "    act_cleaned = []\n",
    "    time_cleaned = []\n",
    "    mesh_cleaned = []\n",
    "\n",
    "    # print('2222df_act_traj', df_act_traj.iloc[0].tolist())\n",
    "    # print('222', df_time_traj.columns)\n",
    "    # sys.exit()\n",
    "\n",
    "    for i in range(len(df_act_traj)):\n",
    "        a_row = df_act_traj.iloc[i].tolist()\n",
    "        t_row = df_time_traj.iloc[i].tolist()\n",
    "        m_row = df_mesh_traj.iloc[i].tolist()\n",
    "        a_new, t_new, m_new = remove_consecutive_duplicates(a_row, t_row, m_row)\n",
    "        act_cleaned.append(a_new)\n",
    "        time_cleaned.append(t_new)\n",
    "        mesh_cleaned.append(m_new)\n",
    "\n",
    "    # データフレーム化して保存\n",
    "    df_act_cleaned = pd.DataFrame(act_cleaned)\n",
    "    df_time_cleaned = pd.DataFrame(time_cleaned)\n",
    "    df_mesh_cleaned = pd.DataFrame(mesh_cleaned)\n",
    "    # print('3', df_mesh_cleaned.head(5))\n",
    "    # sys.exit()\n",
    "\n",
    "    df_mesh_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'meshtraj{nameid_list[datasetid]}.csv'), index=False) \n",
    "    df_time_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'timetraj{nameid_list[datasetid]}.csv'), index=False)\n",
    "    df_act_cleaned.iloc[1:, :].to_csv(os.path.join(input_path, f'acttraj{nameid_list[datasetid]}.csv'), index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce205a",
   "metadata": {},
   "source": [
    "1週間ごとに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基礎関数の用意\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "\n",
    "def split_line_to_segments(line):\n",
    "    \"\"\"\n",
    "    各行のトークン列を '<b>'／'<e>' で分割し、セグメントリストを返す\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    current = []\n",
    "    for token in line:\n",
    "        if token == '<b>':\n",
    "            current = ['<b>']\n",
    "        elif token == '<e>':\n",
    "            if current:\n",
    "                current.append('<e>')\n",
    "                segments.append(current)\n",
    "        else:\n",
    "            current.append(token)\n",
    "    return segments \n",
    "\n",
    "\n",
    "def flatten_chunks(chunks):\n",
    "    \"\"\"\n",
    "    チャンク化されたセグメントを '<b>' と '<e>' 付きのフラットな行リストに変換\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for chunk in chunks:\n",
    "        row = ['<b>']\n",
    "        for seg in chunk:\n",
    "            row.extend(seg)\n",
    "        row.append('<e>')\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def date_of_time_seg(seg):\n",
    "    \"\"\"時間セグメントから日付を抽出\"\"\"\n",
    "    return datetime.strptime(seg[1], \"%Y-%m-%d %H:%M:%S\").date() # 最初のセグメントだけidが入ってるので3番目の項から始まる\n",
    "\n",
    "\n",
    "def group_into_weeks(segments): #, date_extractor):\n",
    "    \"\"\"\n",
    "    7日間ごとにセグメントをグループ化\n",
    "    \"\"\"\n",
    "    if not segments:\n",
    "        return []\n",
    "    chunks = []\n",
    "    current = []\n",
    "    start_date = datetime.strptime(segments[0][2], \"%Y-%m-%d %H:%M:%S\").date() # 最初のセグメントだけidが入ってるので3番目の項から始まる ok\n",
    "\n",
    "    for i, seg in enumerate(segments):\n",
    "        d = date_of_time_seg(seg)\n",
    "        if d < start_date + timedelta(days=7):\n",
    "            current.append(seg)\n",
    "        else:\n",
    "            chunks.append(current)\n",
    "            current = [seg]\n",
    "            start_date = d\n",
    "        \n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def fill_missing_days(weekly_time_segs, weekly_act_segs, weekly_mesh_segs):\n",
    "    \"\"\"\n",
    "    欠落日にはプレースホルダーを挿入（time: 日付トークン / act: '955'）\n",
    "    \"\"\"\n",
    "    # date -> (time_seg, act_seg)\n",
    "    date_map = {}\n",
    "    for t_seg, a_seg, m_seg in zip(weekly_time_segs, weekly_act_segs, weekly_mesh_segs):\n",
    "        date_map[date_of_time_seg(t_seg)] = (t_seg, a_seg, m_seg)\n",
    "\n",
    "    start = date_of_time_seg(weekly_time_segs[0])\n",
    "    filled_time, filled_act, filled_mesh = [], [], []\n",
    "    for i in range(7):\n",
    "        d = start + timedelta(days=i)\n",
    "        if d in date_map:\n",
    "            t_seg, a_seg, m_seg = date_map[d]\n",
    "        else:\n",
    "            t_seg = ['<b>', d.strftime(\"%Y-%m-%d\"), '<e>']\n",
    "            a_seg = ['<b>', \"955\", '<e>']\n",
    "            m_seg = ['<b>', \"999999\", '<e>']\n",
    "\n",
    "        filled_time.append(t_seg)\n",
    "        filled_act.append(a_seg)\n",
    "        filled_mesh.append(m_seg)\n",
    "\n",
    "    return filled_time, filled_act, filled_mesh\n",
    "\n",
    "\n",
    "def write_chunks_to_csv(chunks, output_path):\n",
    "    with open(output_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for segs in chunks:\n",
    "            row = ['<b>'] + [token for seg in segs for token in seg] + ['<e>']\n",
    "            writer.writerow(row[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データセットに対して実行\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "    act_file = (os.path.join(input_path, f'acttraj{nameid_list[datasetid]}.csv'))\n",
    "    time_file = (os.path.join(input_path, f'timetraj{nameid_list[datasetid]}.csv'))\n",
    "    mesh_file = (os.path.join(input_path, f'meshtraj{nameid_list[datasetid]}.csv'))\n",
    "\n",
    "    out_time_path = os.path.join(input_path, f'time_weekly{nameid_list[datasetid]}.csv')\n",
    "    out_act_path = os.path.join(input_path, f'act_weekly{nameid_list[datasetid]}.csv')\n",
    "    out_mesh_path = os.path.join(input_path, f'mesh_weekly{nameid_list[datasetid]}.csv')\n",
    "    \n",
    "    # csvを読み込む：先頭はuseridなので飛ばす\n",
    "    with open(time_file, newline='') as f:\n",
    "        time_rows = list(csv.reader(f))[1:]\n",
    "    with open(act_file, newline='') as f:\n",
    "        act_rows = list(csv.reader(f))[1:]\n",
    "    with open(mesh_file, newline='') as f:\n",
    "        mesh_rows = list(csv.reader(f))[1:]\n",
    "\n",
    "    all_time_chunks, all_act_chunks, all_mesh_chunks = [], [], []\n",
    "    count = 0 \n",
    "    for t_row, a_row, m_row in zip(time_rows, act_rows, mesh_rows): # 個人ごと\n",
    "        t_segs = split_line_to_segments(t_row) # idは消えてる\n",
    "        a_segs = split_line_to_segments(a_row)\n",
    "        m_segs = split_line_to_segments(m_row)\n",
    "\n",
    "        ## これある？？\n",
    "        userid = (t_row[0]) # fixed\n",
    "        weekly_ts = group_into_weeks(t_segs) # 日毎のb-eのリスト\n",
    "        idx_map = {tuple(seg): idx for idx, seg in enumerate(t_segs)}\n",
    "\n",
    "        for week in weekly_ts: # 1週間ごとになっている\n",
    "            # 元データのactを対応づけ\n",
    "            act_week = []\n",
    "            mesh_week = []\n",
    "            for seg in week:\n",
    "                idx = idx_map.get(tuple(seg))\n",
    "                act_week.append(a_segs[idx] if idx is not None else [\"955\"])\n",
    "                mesh_week.append(m_segs[idx] if idx is not None else [\"999999\"])\n",
    "\n",
    "            # 欠落日の補完\n",
    "            filled_t, filled_a, filled_m = fill_missing_days(week, act_week, mesh_week)\n",
    "\n",
    "            filled_t[0].insert(0, f'{userid}')\n",
    "            filled_a[0].insert(0, f'{userid}')\n",
    "            filled_m[0].insert(0, f'{userid}')\n",
    "\n",
    "            all_time_chunks.append(filled_t)\n",
    "            all_act_chunks.append(filled_a)\n",
    "            all_mesh_chunks.append(filled_m)\n",
    "\n",
    "    write_chunks_to_csv(all_time_chunks, out_time_path)\n",
    "    write_chunks_to_csv(all_act_chunks, out_act_path)\n",
    "    write_chunks_to_csv(all_mesh_chunks, out_mesh_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe9e08",
   "metadata": {},
   "source": [
    "長さを揃えてから，5つのデータを統合する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧱 総行数: 3992, 3992, 3992\n"
     ]
    }
   ],
   "source": [
    "act_all_rows = [] \n",
    "time_all_rows = [] \n",
    "mesh_all_rows = [] \n",
    "\n",
    "act_max_cols = 0\n",
    "time_max_cols = 0\n",
    "mesh_max_cols = 0\n",
    "\n",
    "for datasetid, dataset in enumerate(dataset_list):\n",
    "    base_path = os.path.join(base_path0, dataset)\n",
    "    folderlist = os.listdir(base_path)\n",
    "\n",
    "    time_path = os.path.join(input_path, f'time_weekly{nameid_list[datasetid]}.csv')\n",
    "    act_path = os.path.join(input_path, f'act_weekly{nameid_list[datasetid]}.csv')\n",
    "    mesh_path = os.path.join(input_path, f'mesh_weekly{nameid_list[datasetid]}.csv')\n",
    "\n",
    "    # 読み込み + 空文字・None・\"NaN\"除去\n",
    "    with open(act_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        act_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "    \n",
    "    with open(time_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        time_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "\n",
    "    with open(mesh_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        mesh_cleaned_rows = [[cell for cell in row if cell not in [None, '', 'NaN']] for row in reader]\n",
    "    \n",
    "    # 最大列数を更新\n",
    "    for row in act_cleaned_rows:\n",
    "        act_max_cols = max(act_max_cols, len(row))\n",
    "    \n",
    "    for row in time_cleaned_rows:\n",
    "        time_max_cols = max(time_max_cols, len(row))\n",
    "\n",
    "    for row in mesh_cleaned_rows:\n",
    "        mesh_max_cols = max(mesh_max_cols, len(row))\n",
    "    \n",
    "    act_all_rows.extend(act_cleaned_rows)\n",
    "    time_all_rows.extend(time_cleaned_rows)\n",
    "    mesh_all_rows.extend(mesh_cleaned_rows)\n",
    "\n",
    "# 行の長さを最大列数にそろえ、<p>で埋める\n",
    "act_padded_rows = [row + ['<p>'] * (act_max_cols - len(row)) for row in act_all_rows]\n",
    "time_padded_rows = [row + ['<p>'] * (time_max_cols - len(row)) for row in time_all_rows]\n",
    "mesh_padded_rows = [row + ['<p>'] * (mesh_max_cols - len(row)) for row in mesh_all_rows]\n",
    "\n",
    "# 結合結果を保存\n",
    "act_output_path = os.path.join(input_path, 'act_weekly_filled.csv')\n",
    "time_output_path = os.path.join(input_path, 'time_weekly_filled.csv')\n",
    "mesh_output_path = os.path.join(input_path, 'mesh_weekly_filled.csv')\n",
    "\n",
    "with open(act_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(act_padded_rows)\n",
    "\n",
    "with open(time_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(time_padded_rows)\n",
    "\n",
    "with open(mesh_output_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(mesh_padded_rows)\n",
    "\n",
    "print(f\"🧱 総行数: {len(act_padded_rows)}, {len(time_padded_rows)}, {len(mesh_padded_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f1e77a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/qgnv6b3961q5nz5l1v78bw080000gn/T/ipykernel_36207/1425381383.py:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_time = df_time.applymap(lambda x: convert_datetime_to_hour(x) if pd.notna(x) else x)\n"
     ]
    }
   ],
   "source": [
    "def convert_datetime_to_hour(token):\n",
    "    try:\n",
    "        # 時刻が含まれる場合（フォーマット付き日付）\n",
    "        dt = pd.to_datetime(token, errors='raise')\n",
    "        if dt.hour == 0 and \"00:00:00\" not in token:\n",
    "            return 25 # token  # たとえば \"2022-12-21\" のような場合\n",
    "        return str(dt.hour)\n",
    "    except Exception:\n",
    "        # 変換できない場合はそのまま返す\n",
    "        return token\n",
    "## \n",
    "df_time = pd.read_csv(os.path.join(input_path, 'time_weekly_filled.csv'), header=None)\n",
    "df_time = df_time.applymap(lambda x: convert_datetime_to_hour(x) if pd.notna(x) else x)\n",
    "df_time.to_csv(os.path.join(input_path, 'time_weekly_filled_hour.csv'), index=False, header=False)\n",
    "# converted_row = [convert_datetime_to_hour(tok) for tok in row]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fcf11",
   "metadata": {},
   "source": [
    "日付変更時点をSトークンに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0f8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act = pd.read_csv(os.path.join(input_path, 'act_weekly_filled.csv'), header=None)\n",
    "df_time = pd.read_csv(os.path.join(input_path, 'time_weekly_filled_hour.csv'), header=None)\n",
    "df_mesh = pd.read_csv(os.path.join(input_path, 'mesh_weekly_filled.csv'), header=None)\n",
    "\n",
    "# 各行見ていって，<b>を消す＆<e>を<S>に置換\n",
    "df_act = df_act.replace('<e>', '<s>')\n",
    "df_time = df_time.replace('<e>', '<s>')\n",
    "df_mesh = df_mesh.replace('<e>', '<s>')\n",
    "\n",
    "# 特定の文字を削除して左詰め（文字列結合で再構成）\n",
    "def remove_and_shift(row, char='<b>'):\n",
    "    # 各セルから特定文字を削除し、行を再構成\n",
    "    cleaned = [str(cell).replace(char, '') for cell in row]\n",
    "    # 空でないものを左に寄せ、空文字を右に詰める\n",
    "    non_empty = [c for c in cleaned if c]\n",
    "    empty = [''] * (len(row) - len(non_empty))\n",
    "    return pd.Series(non_empty + empty)\n",
    "\n",
    "# 各行に適用\n",
    "df_act_cleaned = df_act.apply(remove_and_shift, axis=1)\n",
    "df_time_cleaned = df_time.apply(remove_and_shift, axis=1)\n",
    "df_mesh_cleaned = df_mesh.apply(remove_and_shift, axis=1)\n",
    "\n",
    "df_act_cleaned = df_act_cleaned.fillna('<p>')\n",
    "df_time_cleaned = df_time_cleaned.fillna('<p>')\n",
    "df_mesh_cleaned = df_mesh_cleaned.fillna('<p>')\n",
    "# print(df_act_cleaned)\n",
    "\n",
    "df_act_cleaned = df_act_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "df_time_cleaned = df_time_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "df_mesh_cleaned = df_mesh_cleaned.replace(['', 'NaN'], '<p>') #, inplace=True)\n",
    "\n",
    "df_act = df_act_cleaned\n",
    "df_time = df_time_cleaned\n",
    "df_mesh = df_mesh_cleaned\n",
    "\n",
    "#### ここから <s> を <s1>, <s2>, ... に変換 ####\n",
    "def convert_d(row):\n",
    "    # row ごとにカウント変えるので\n",
    "    count = 0\n",
    "    for i in range(len(row)):\n",
    "        if row[i] == '<s>':\n",
    "            count += 1\n",
    "            if count <= 6: # countが7, 8の時は一番後ろのやつなので無視する\n",
    "                row[i] = '<s' + str(count) + '>'\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return row\n",
    "\n",
    "for i in range(len(df_time)):\n",
    "    # 1行ずつ取り出す\n",
    "    row = df_time.iloc[i]\n",
    "    # 1行の中に<s>がいくつあるかをカウント\n",
    "    row = convert_d(row)\n",
    "    # 変換した行を元のデータフレームに戻す\n",
    "    df_time.iloc[i] = row\n",
    "\n",
    "for i in range(len(df_act)):\n",
    "    # 1行ずつ取り出す\n",
    "    row = df_act.iloc[i]\n",
    "    # 1行の中に<s>がいくつあるかをカウント\n",
    "    row = convert_d(row)\n",
    "    # 変換した行を元のデータフレームに戻す\n",
    "    df_act.iloc[i] = row\n",
    "\n",
    "for i in range(len(df_mesh)):\n",
    "    # 1行ずつ取り出す\n",
    "    row = df_mesh.iloc[i]\n",
    "    # 1行の中に<s>がいくつあるかをカウント\n",
    "    row = convert_d(row)\n",
    "    # 変換した行を元のデータフレームに戻す\n",
    "    df_mesh.iloc[i] = row\n",
    "\n",
    "df_time.to_csv(os.path.join(input_path, 'time_weekly_filled_structured.csv'), index=False)\n",
    "df_act.to_csv(os.path.join(input_path, 'act_weekly_filled_structured.csv'), index=False)\n",
    "df_mesh.to_csv(os.path.join(input_path, 'mesh_weekly_filled_structured.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbef81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
